% STEP 1: Choose oneside or twoside. Use the 'draft' option a lot when writing.
\documentclass[english, oneside]{HYgradu}

\usepackage[utf8]{inputenc} % For UTF8 support. Use UTF8 when saving your file.
\usepackage{lmodern} % Font package
\usepackage{textcomp}
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
%\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage[sort,colon]{natbib} % For bibliography
\usepackage[bf]{caption} % For more control over figure captions
\usepackage{marginnote} % margin notes for myself
%\usepackage{svg} % vector graphics
\usepackage{graphicx}
\graphicspath{{kuvat/tex/}}
\usepackage{wasysym}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{makecell}
\usepackage{rotating}
\usepackage[shortcuts]{extdash}

% titlesec and fix for no numbering on titlesec 2.10.2
\usepackage{titlesec}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
\patchcmd{\ttlh@hang}{\noindent}{}{}{}
\makeatother

\fussy % Probably not needed but you never know...

\renewcommand{\topfraction}{.75} % less single-float pages
\renewcommand{\floatpagefraction}{.75} % less single-float pages

\newcommand{\matr}[1]{\mathbf{#1}}

% näillä subsubsubsection käyttöön
%\titleclass{\subsubsubsection}{straight}[\subsection]
%\newcounter{subsubsubsection}[subsubsection]
%\renewcommand\thesubsubsubsection{\thesubsection.\arabic{subsubsubsection}}
%\titleformat{\subsubsubsection}
%  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
%\titlespacing*{\subsubsubsection}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
%\makeatletter
%\def\toclevel@subsubsubsection{3}
%\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
%\@addtoreset{subsubsubsection}{section}
%\@addtoreset{subsubsubsection}{subsection}
%\makeatother
%\setcounter{secnumdepth}{3}
%\setcounter{tocdepth}{4}

% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
%\hypersetup{
%    bookmarks=true,         % show bookmarks bar first?
%    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
%    pdftoolbar=true,        % show Acrobat’s toolbar?
%    pdfmenubar=true,        % show Acrobat’s menu?
%    pdffitwindow=false,     % window fit to page when opened
%    pdfstartview={FitH},    % fits the width of the page to the window
%    pdftitle={},            % title
%    pdfauthor={},           % author
%    pdfsubject={},          % subject of the document
%    pdfcreator={},          % creator of the document
%    pdfproducer={pdfLaTeX}, % producer of the document
%    pdfkeywords={something} {something else}, % list of keywords for
%    pdfnewwindow=true,      % links in new window
%    colorlinks=true,        % false: boxed links; true: colored links
%    linkcolor=black,        % color of internal links
%    citecolor=black,        % color of links to bibliography
%    filecolor=magenta,      % color of file links
%    urlcolor=cyan           % color of external links
%}

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Your Title Here}
\author{Anni Järvenpää}
\date{\today}
\level{Master's thesis}
\faculty{Faculty of Science}
\department{Department of Physics}
\address{PL 64 (Gustaf Hällströmin katu 2a)\\00014 University of Helsinki}
\subject{Astronomy}
\prof{Professor Peter Johansson}{Dr. Till Sawala}
\censors{prof. Smith}{doc. Smythe}{}
\depositeplace{}
\additionalinformation{}
\numberofpagesinformation{\numberofpages\ pages}
\classification{}
\keywords{Your keywords here}
\quoting{}%``Bachelor's degrees make pretty good placemats if you get them laminated.'' \\---Jeph Jacques}
% There is a way out of every box, a solution to every puzzle; it's just a matter of finding it
% Things are only impossible until they're not

\begin{document}

% Generate title page.
\maketitle
% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.

\begin{abstract}
Abstract goes here.
\end{abstract}

% Place ToC
\mytableofcontents

% set line spacing to the publication-ready single spacing:
%\singlespacing

% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Introduction}

\section{TL;DR version of prerequisite information}
\begin{enumerate}
	\item galaxies form
	\begin{itemize}
		\item Why?
		\item When?
		\item How?
		\item Where?
	\end{itemize}
	\item galaxies form in groups
	\item our local group is one of these
	\item something about large scale distribution of galaxies
\end{enumerate}

\section{History of Local Group Research}
LG objects visible with naked eye -> realization they are something outside our galaxy -> realization they are something very much like our galaxy

First determining distance was difficult, now mass is more interesting question

\section{Aim of This Thesis}
Whatever the main results end up being, presented in somewhat coherent manner and hopefully sugar-coated enough to sound Important and Exciting.


\chapter{Theoretical Background}
%Think whether LG or LCDM first
%theorythingies from Fattahi paper introduction

Cosmology determines the properties of the Universe, including its origin, the rules by which it evolves and the structures that arise \citep{mo2010galaxy}. Thus many fields of astronomy and astrophysics, including the study of galaxies and galaxy groups such as the Local group and its members, is tightly connected to the study of cosmology. This section gives a brief explanation of the current cosmological understanding, its relevant implications and how the Local Group is currently viewed in the cosmological context.

\section{Basics of Cosmology}
Current cosmological understanding is based on a subset of the general theory of relativity together with simple hypotheses such as the cosmological principle, which states that on large scales the Universe is spatially homogeneous and isotropic \citep{mo2010galaxy}.

%\subsection{Discovery}
%Make maths, add cosmological constant, make observations, remove cosmological constant

%Enough cosmology here or in other sections to make other parts of thesis to make sense and to suffice as master's thesis = basic textbook cosmology and galaxy formation theory %t. puppe


\subsection{Hubble flow}
What is, where seen, what means, how to measure, hotness/coldness

Plot: observations with fitted hubble flow

\subsection{$\Lambda$CDM Cosmology}
% nämä siirretty toisaalta, johon olin aloittamassa. tuikeiden galaksien kirja tai galaxies and cosmology parhaat lähdeveikkaukset ehkä, sparken s. 172 lyhyesti hyvä
%For the first 380~000 years in the evolution in the timeline of the Universe, temperature of the universe was high enough to keep hydrogen and helium ionized, and thus the Universe consisted of plasma where radiation could not move freely, which meant that matter and radiation were bound together.
%After the temperature of the radiation filling the Universe no longer had the energy to keep hydrogen and helium ionized at around 380~000 years after the Big Bang, the Universe became transparent and the gas started to accrete and collapse %TODO kerro paremmin, aloita siitä että pimeä aine klimppiintyy aiemmin? galkos 13 s 15 (pdf)



\section{Local Group}
%%Definition of galaxy group, our local group is one of these.
%
%%Only few galaxies are born and evolve alone, and the Milky Way is not an exception \citet{longair2008galaxy}.
%
%\reversemarginpar
%\marginnote{\footnotesize{vastaa kiusallisen hyvin introductionin suunnitelmaa, siirrä sinne myöhemmin?}}
%Our galaxy is one of the numerous galaxies that are a part of a galaxy group. According to \citet{sparke2007galaxies}, around half of all galaxies are part of either a cluster or a group of galaxies where gravitational attraction of matter within the structure is strong enough to dominate over expansion of the universe. The two are distinguished by the mass of the association and number of objects in it: \citeauthor{sparke2007galaxies} define systems with more than 50 luminous galaxies within a megaparsec from the center of the system as galaxy clusters whereas groups of galaxies have fewer members and often have masses less than $10^{14}\ M_{\astrosun}$.
%
%\reversemarginpar
%\marginnote{\footnotesize{tällä hetkellä tosi vähän siitä, miten esim etäisyyksiä mitataan, pitäisikö olla enemmän?}}
%The Local Group with its three bright spirals and numerous smaller galaxies scattered within roughly a megaparsec from the Milky Way and the Andromeda galaxy is a fairly typical galaxy group \citep{sparke2007galaxies}. 
%Due to proximity of the members of the group, many objects can be resolved as individual stars and even very faint objects can be detected, which makes the Local Group an excellent test laboratory to study interactions between galaxies. On the other hand, our location within the Milky Way makes estimating the brightness and other properties of our own galaxy difficult and the extinction at the plane of the galaxy might obscure some faint members of the group \citep{sparke2007galaxies}.
%
%\reversemarginpar
%\marginnote{\footnotesize{tämä ei varmaan oikeasti johdattele sisältöön kovin hyvin, mieti sopivuus kun varsinaiset kappaleet ovat valmiita}}
%The following sections shortly describe the structure, kinematics and evolution of the Local Group with focus on the Milky Way and Andromeda galaxy pair, as these are the most prominent members and their coevolution is the most defining process in the Local Group. Their mutual dynamics also offer one way of estimating the mutual mass of the galaxy pair, which [jossain osassa esitellään, jossain arvioidaan vs muut menetelmät]. %TODO
%Many of the smaller members of the group are also satellites for either of the big galaxies and will eventually merge into their host. 
%
%
%
%%Mass estimate (Li, Yang masses for the LG and MW)
%
%%Maybe something about scale of things in our universe, what are galaxy groups made of, what do you get if you go one distance scale up, what's different in galaxy clusters
%
%%\subsection{Structure}
%\subsection{Spiral Galaxies}
%\reversemarginpar
%\marginnote{\footnotesize{jos jatkossakin sparkea, mainitse leipätekstissä ja skippaa yksittäiset viittaukset. \\Ajantasaisempi lähde galaksien määrälle?}}
%Only three out of the more than 30 galaxies in the Local Group are spiral galaxies: the Andromeda Galaxy, its satellite M33 and our home galaxy, the Milky Way. These three galaxies dominate the Local Group in multiple ways. First of all, the three spirals are the three most massive galaxies in the Local Group \citep{sparke2007galaxies}. The three galaxies also emit a total of 90~\% of all visible light of the Local Group, Andromeda being the most luminous with its $27 \times 10^9~L_{\astrosun}$ V-band luminosity, followed by the Milky Way with its $15\times 10^9~L_{\astrosun}$ and the M33 at $5.5 \times 10^9~L_{\astrosun}$ \citep{sparke2007galaxies}. Due to the Milky Way and the M33 being the most massive members of the Local Group, the center of the local group is often considered to lie between the Milky Way and the Andromeda galaxy \citep{sparke2007galaxies}.
%
%\reversemarginpar
%\marginnote{\footnotesize{DM mainitaan vain ohimennen, enemmän? Kerrotko ollenkaan, että MW \& M31 matkalla kohti toisiaan?}}
%The three spirals of the group are fairly typical spiral galaxies with a slightly warped thin disk of stars, gas and dust with a central bulge, surrounded by globular clusters and residing in a dark matter halo.
%However, there are some differences between the galaxies in addition to M33 being considerably less luminous than the other two. Whereas bulge and core of M33 are very small relative to the size of the galaxy compared to ones in the Milky Way and Andromeda galaxy, the latter differ in the contents of the core \citep{sparke2007galaxies}. In the core of the Milky Way, gas and dust accreting into the central black hole power the radio source Sagittarius~A* while the core of the Andromeda galaxy contains very little interstellar matter and M33 has a very low mass black hole or none at all \citep{sparke2007galaxies}.
%
%The spiral patterns of the galaxies also differ and thus the galaxies belong to different classes in the Hubble sequence, Milky way being an Sbc galaxy, the Andromeda galaxy falling in class Sb and M33 in Sc \citep{sparke2007galaxies}. The Andromeda galaxy has tightly wound spiral arms, which makes seeing the spiral pattern very difficult and star-formation is concentrated in a 'ring of fire' encircling the center of the galaxy at about 10 kpc distance \citep{sparke2007galaxies}. M33 on the other hand has more open spiral arms where clumps of newly formed stars are clearly visible \citep{sparke2007galaxies}.
%
%\reversemarginpar
%\marginnote{\footnotesize{tynkä kappale}}
%The mutual gravitational attraction of the system is strong enough to locally overcome the expansion of the universe and thus we observe the Andromeda Galaxy having a velocity of about 110 km/s towards the Milky Way \citep{vandermarel2012m31}. This is also what allows the Milky Way and Andromeda to maintain a system of smaller satellite galaxies around them.
%
%\reversemarginpar
%\marginnote{\footnotesize{massa tänne vai toisaalle?}}
%TODO: mass estimate %TODO
%
%
%\subsection{Other Members of the Local Group}\label{LGothers} %TODO sijainti tasossa
%Despite spirals dominating the local group with their mass, most of the galaxies in the Local Group are either dwarf spheroidals or irregular galaxies \citep{sparke2007galaxies}. In fact, out of the 35 non-spiral members
%the Local Group listed by \citet{sparke2007galaxies} only one is an elliptical galaxy, rest of the galaxies being either dwarf spheroidals or irregular galaxies. This elliptical is a small satellite of the Andromeda galaxy, visible near or on top of the disk of the Andromeda Galaxy in many optical images of the galaxy.
%
%Current list of Local Group members might not be complete though as it is possible that some members of the Local Group have not yet been discovered due to the heavy absorption in the plane of the Milky Way.\citep{sparke2007galaxies}. New ultra-faint galaxies are also still discovered, for example Leo III in 2015 \citep{kim2015hero} and some of the eight companions discovered by \citet{bechtol2015eight}.
%
%Most of the other galaxies in the Local Group are satellite galaxies of either the Milky Way or the Andromeda galaxy: both of them have about a dozen of known satellites that orbit the primary \citep{sparke2007galaxies}.  In addition to these, \citet{sparke2007galaxies} list 14 'free fliers' that are within 1 Mpc of the center of the Local Group but do not lie close to any large galaxies. The M33 is also speculated to possibly have its own satellites  \citep{martin2009pandas, chapman2013dynamics}.
%
%For the Milky Way, two most prominent satellites are the Large and Small Magellanic Clouds, a pair of irregular galaxies that are also the most massive galaxies in the Local Group after the three spirals. The galaxies appear as irregular cloud-like structures near each other on the southern sky \citep{sparke2007galaxies}. Despite the irregular shape of the galaxies, the Large Magellanic Cloud is often considered to be a Magellanic spiral (SBm) instead of an irregular galaxy as it has a bar and one short spiral arm but no other characteristics of a spiral galaxy \citep{sparke2007galaxies}. Both galaxies are rich in hydrogen and star clusters with evidence of active star-formation history, but curiously the Large Magellanic Cloud has very few stellar clusters with ages from four to ten Gyr whereas Small Magellanic Cloud does not have a similar gap in its star-formation history \citep{sparke2007galaxies}. The galaxies are also connected by a bridge of gas and young star clusters and 
%
%Both the irregular shapes and the curious star formation histories are likely results of interactions between galaxies. The Magellanic clouds are on orbits that bring them close to the Milky Way and them appearing near each other on the sky is not by chance: the galaxies are also on orbits around each other \citep{sparke2007galaxies}. These interactions have created a bridge of gas and young stars connecting the Magellanic clouds and multiple large gas clouds share the orbit of the Large Magellanic Cloud on its way around the Milky Way, seen as a stream of gas stretching around the sky as a more than 100\textdegree\ arc \citep{sparke2007galaxies}. This gas is likely to have been ripped from the Large Magellanic Cloud by the Small Magellanic Cloud when they passed near each other on their orbit \citep{sparke2007galaxies}.
%
%\subsection{Evolution}
%The Local Group is a dynamic system where the interplay of its members have affected the system in many ways that we can now observe, as will the composition of the group further change in the future when the gravitational interactions continue to shape the system \citep{sparke2007galaxies}. The origins of the Local Group, as origins of all structure in our visible universe, lie in density perturbations in the early Universe, denser regions collapsing first followed by gradually less and less dense ones \citep{sparke2007galaxies}. Further, the perturbations occur on multiple scales: the Local Group as a whole is overdense, but it consists of smaller but even denser clumps separated with areas where matter is more sparse \citep{sparke2007galaxies}.
%
%\reversemarginpar
%\marginnote{\footnotesize{lyhyt, soossi}}
%The baryonic matter alone is not able to explain the structures we observe today: as discussed in section [TODO], %TODO 
%structure formation requires substantially more mass than is observed with electromagnetic radiation. Thus the processes discussed in this section are dominated by the dark matter content of the Universe with the baryonic matter merely following the gravitational lead of the dark matter.
%
%The collapse of these denser areas that would later form the Local Group started with the collapse of the oversende clumps that would simultaneously fall towards the center of the larger-scale overdense region \citep{sparke2007galaxies}. This results in some of the most central clumps merging and forming a single galaxy which other clumps forming smaller satellites around it \citep{sparke2007galaxies}. This all happens on multiple scales, as for example in the Local Group Milky Way and the Andromeda Galaxy fall towards each other while both of them have their satellite galaxies, some of which are speculated to have their own satellite systems as mentioned in section \ref{LGothers}.
%
%\reversemarginpar
%\marginnote{\footnotesize{lyhyt}}
%These clumps of matter are irregularly shaped, which results in gravitational interactions between the clumps also applying torque to the matter and thus setting the matter in rotation \citep{sparke2007galaxies}. The infall of the matter due to gravity and collisions then further increases the rotational velocity as the angular momentum is approximately conserved in the collisions \citep{sparke2007galaxies}.
%
%One way of gaining insight into the evolution of matter in protogalaxies that would later form the galaxies in our Local Group is to observe the Milky Way, where observers can resolve many structures in greater precision than in any other place in the Universe. For example the metal-poor globular clusters encircling the Milky Way tend to each consist of stars with similar composition, which suggests that the globulars were born of small homogenous clouds of gas early in time, enriched by just a few supernova explosions \citep{sparke2007galaxies}. The collapse of the gas into a cluster of stars may have begun either after a perturbance of some sort, such as a collision with another cloud, or the cloud just exceeds the Jeans mass \citep{sparke2007galaxies}.
%
%Many of the globlulars are also on elongated orbits around the center of the Milky Way, which further suggests that they formed at an early time: as dissipates its kinetic energy in interactions much more efficiently than stars that almost never actually collide \citep{sparke2007galaxies}. Thus the later a system has transformed from gaseous to mainly stellar, the more ordered rotation we observe. This can be seen both in the disks and haloes of spiral galaxies: the rotation in the thin disk, containing the youngest stars, is very ordered and the stars are on closely aligned nearly circular orbits, whereas thick disk that contains slightly older stars also has more scatter in their orbits, and the old stars of metal-poor halo have elongated orbits in random orientations \citep{sparke2007galaxies}.
%
%\reversemarginpar
%\marginnote{\footnotesize{käytännössä kaikki käyttävät muotoa km~s$^{-1}$, olisiko parempi?}}
%The angular momentum of the stars also gives valuable information about the extent of the cloud of gas from which the stars and thus the galaxy have formed. According to \citet{sparke2007galaxies}, simulations suggest that on average particles in a protogalaxy attain about 5\% of the velocity of a circular orbit at the radius of the particle. Now if the rotation curve and thus the potential of the modern-day galaxy is known, one can estimate the original extent of the gas cloud from the change of the angular momentum as the particle moves inward in the potential field \citep{sparke2007galaxies}. Using a flat rotation curve with rotation velocity of 200~km/s, \citet{sparke2007galaxies} estimates that the gas has been accreted from around 100~kpc radius, an area much larger than the current about 15~kpc extent of the stellar disk.
%
%% nyt MW-osan contenttiin
%
%
%M32 ehkä galaksin ydin
%How have we ended up in a situation described earlier? What will happen in future?

\subsection{Determining the Mass of the Local Group and Its Members}

% dark matter haloes merging a bit?

%Galaxies that are part of LG, distribution of smaller ones around bigger ones

%Current mass estimates (at least timing argument, hubble flow and maybe satellites)



\chapter{Simulations and Simulation Codes}
%Data used here from EAGLE which uses modified GADGET-2 which is a tree-code that uses leapfrog


%TODO : sweeping statements, view plane bullshit, paremmat syyt se että havainnoista ei nähdä DM ja analyyttiset mallit eivät riitä kaikkeen. ehkä koko kappale uusiksi?
Simulations are a valuable tool in astrophysical research as they bypass many major restrictions characteristic to observational astronomy such as not being able to observe the evolution of a single object with the exception of the most rapid events such as supernovae, and the observations being constrained to a single view plane. The data used in this master's thesis also originates from a cosmological N\=/body simulation. This section shortly introduces first some fundamental operational principles of N\=/body simulations and then specifics of the [insert name or other identification here] %TODO
simulations from which the data used here originates from.


\section{N-body simulations}
N\=/body simulations are a type of computer simulations that follow a number of particles interacting with each other, often used in computational astrophysics \citep{binney2008galactic}. Their concept is simple: the current states of the particles are known and a physical model is used to calculate how the states of the particles should advance over a small period of time known as the time step \citep{binney2008galactic}. Simple dark matter only simulations might only handle gravitational interactions of the particles, but many simulation codes such as GADGET-2 \citep{springel2005cosmological} (see section \ref{sect:gadget} for more about GADGET family simulation codes) and Enzo \citep{norman2007simulating} are also able to handle hydrodynamics of baryonic matter and can include star formation, feedback and other baryonic processes.

There are multiple ways to handle both the force calculations and calculating new positions for the particles \citep{binney2008galactic}. For the force calculations most popular algorithms are based on either hierarchical trees or particle meshes, but for calculating the positions, a wide variety of integrators have been developed for a range of different needs \citep{binney2008galactic}. The [insert identification here] %TODO
simulations are run using a modified GADGET-3 and thus use the TreePM method for force calculations accompanied by a leapfrog integrator. TreePM is a mix of a hierarchical tree for close range forces and a particle mesh for distant forces, the former of which is more interesting. Therefore the \citet{barnes1986hierarchical} hierarchical tree algorithm is introduced in the following subsection, followed by a short description of the leapfrog integrator in the next one.

\subsection{Hierarchical Tree Algorithm} \label{sect:tree}
In many applications of computational astrophysics the desired number of particles in a simulation is too great to allow calculating interparticle forces by direct summation as its time complexity is $\mathcal{O}(n^2)$. One of the alternatives is organizing the particles into a tree data structure, which allows distant particles residing close to each other to be approximated as single more massive particle. This approach was first introduced by \citet{appel1985efficient} and \citet{barnes1986hierarchical}, the latter of which will be followed here.

Constructing the tree starts with setting the full simulation box as the root of the tree. This root cube is then subdivided into eight equally sized sub-cubes called cells. These eight cells are children of the root node. This starts a recursive process where each new cell is again divided into eight subcells recursively until each of the cells contains either no particles, one particle or eight subcells. When a new cell is created, the total mass of the enclosed particles and the location of the mass centre of the particles within it is stored as a pseudoparticle to allow easy calculation of the approximate gravitational effect the particles within a cell have on a distant particle.

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/tree-box.pdf_tex}
    \caption{A two-dimensional example of particles (black dots) being split into cells using the Barnes-Hut algorithm \citep{barnes1986hierarchical}. The outermost square is the simulation box, i.e. the root of the tree, and the samaller squares with decreasing line thicknesses are its descendants. Note how each cell contains either one particle, no particles or four smaller subcells.}\label{fig:tree-box}
\end{figure}
\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{kuvat/tex/tree.pdf_tex}
    \caption{Cells of figure \ref{fig:tree-box} shown as a tree, each level of refined cells in \ref{fig:tree-box} traversed from left to right and up to down corresponding to each level of nodes from left to right. Cells with no particles are not used in force calculations and thus they do not need to be stored in computer memory, but they are shown here to emphasize the structure of the tree.}\label{fig:tree}
\end{figure}
\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{kuvat/tex/tree-solids.pdf_tex}
    \caption{Alternative tree where cells with particles are solid, doesn't work as well as it could}\label{fig:tree-solids}
\end{figure}

To aid understanding the process, a two-dimensional simplification of the simulation box after the tree has been constructed is shown in figure \ref{fig:tree-box}, together with the corresponding tree in figure \ref{fig:tree}. The thick outer line of the former figure is the simulation box, corresponding to the topmost node of the tree in the latter. This root cell is first divided into four (in contrast to eight in the three-dimensional case) subcells, which is shown with the second-thickest line dividing the simulation box into quarters in figure \ref{fig:tree-box}. These four cells are the four children of the root node in the tree. Each of the subcells contains more than one particle, so each of those is again split into four quarters, each quarter a child of the original node in the tree. These quarters of quarters form the third level of the tree in figure \ref{fig:tree}.

Some of these newly-created cells are empty or only have a single particle, so for them the recursion halts and the nodes of the tree are leaves. The rest are again split and a new layer of the tree is created, but as some cells were complete already the fourth layer of the tree is not full. In this case only one cell requires division beyond the fourth level, producing the last four leaves of the tree on the fifth level.

Often, as is the case in the example, some of the leaf nodes are empty. In the case of a real simulation, these of course do not need to be saved as there is no information to store, but in this example case all are drawn to emphasize the regular structure of the tree. In the three-dimensional case where each internal cell has eight subcells the formed tree is known as an octree, whose two-dimensional analog, the quadtree, is constructed in the previous example.

The tree can then be utilized to speed up the force calculations. When calculating the gravitational acceleration felt by a single particle, the tree is traversed starting from the root. The ratio $l/D$ of the length of a side of the cell and the distance from the particle to the pseudoparticle representing the mass within the cell is then calculated. If the ratio is smaller than a predefined accuracy parameter $\theta$ representing the opening angle of the cell, the cell is treated as if everything within it was replaced with the pseudoparticle having the combined mass of the cell. Otherwise the subcells of the cell are examined recursively in the same way until a small enough subcell is found or a leaf of the tree is reached, at which point the pseudoparticle and the real particle in the cell are equivalent and the force can be calculated with the maximum accuracy possible for the simulation.

Using the Barnes-Hut algorithm, the tree construction and the force calculations both have time complexity of $\mathcal{O}(n \log n)$. This is a significant improvement over the $\mathcal{O}(n^2)$ of the direct summation considering that the accuracy cost is fairly small: \citet{barnes1986hierarchical} report accuracy of about 1~\% when $\theta=1$ and the accuracy can be improved by either setting a smaller $\theta$ or including multipole moments in the pseudoparticles \citep{barnes1989error}. Similar algorithms with even better time complexity of $\mathcal{O}(n)$ have also been introduced such as ones by \citet{dehnen2002hierarchical} and \citet{xue1998n}.

The algorithm is also straightforward to parallelize as different branches of the tree can be assigned to their own threads, though memory management has to be done carefully when forces outside the current branch of a thread are calculated \citep{binney2008galactic}. One way to circumvent the memory management issue is to use a particle mesh based integrator for long range forces as GADGET-2 does \citep{springel2005cosmological}. The algorithm can also handle problems where the range of densities in the simulation box is wide which is important for applications such as galaxy mergers and cosmological simulations. This, together with their competitive time complexity, makes tree based codes an appealing tool for many astrophysical simulations \citep{binney2008galactic}.


\subsection{Leapfrog Integrator} \label{sect:leapfrog}
A number of different integrators suitable for astronomical and astrophysical applications have been developed  for solving different problems \citep{binney2008galactic}. No integrator is optimal for every task and thus factors such as the integration time, amount of memory available per particle, smoothness of the potential and the cost of a single gravitational field evaluation should be considered \citep{binney2008galactic}. One of the integrators that are well suited for cosmological simulations is the leapfrog integrator, which is also used by GADGET-2 simulation code among others \citep{springel2005cosmological}.

When a fixed time-step is used, the leapfrog integrator conserves the energy of the system and is time reversible \citep{binney2008galactic}. While a variable time-step is possible and often used, it requires some modifications to the algorithm, presented in e.g. \citet{springel2005cosmological}. Other benefits of the integrator are its second order accuracy and the fact that it doesn't require excessive amounts of memory per particle as only the current state of the system is needed in calculating the next step\citep{binney2008galactic}. With its second order accuracy paired with symplecticity, it rivals fourth order integrators such as the fourth order Runge-Kutta when used for long simulation runs \citep{binney2008galactic}.


% won't work for an unknown reason
%\begin{figure}
%    \centering
%    %\def\svgwidth{\columnwidth}
%    \input{kuvat/tex/leapfrog.pdf_tex}
%    \caption{adsf}\label{fig:leapfrog}
%\end{figure}

\begin{figure}
\centering
\includegraphics{kuvat/tex/leapfrog.pdf}
\caption{Timesteps taken by the leapfrog algorithm, positions ($x$) updated as indicated with lower arrows and velocities ($v$) as the upper arrows. It is not important for the algorithm which of the two is chosen to step through the integer times, choice of it being $x$ in this figure is arbitrary. The dashed short arrows depict the half-steps that are needed when a synchronized output is desired.}\label{fig:leapfrog}
\end{figure}

Timestepping with the leapfrog integrator consists of two phases, the drift and the kick steps, which are alternated with a half-step offset as shown in figure \ref{fig:leapfrog} \citep{binney2008galactic}. During the kick step, the momenta of the particles are updated and during the drift step the positions of the particles are changed according to the previously calculated momenta \citep{binney2008galactic}. When synchronized output of both positions and velocities is desired, a half-timestep advance or backtrack to either of the variables will result in both variables being outputted at the same time. This kind of synchronization steps at the ends of the integration are indicated in figure \ref{fig:leapfrog} with dashed arrows.




\subsection{Halo Finding} \label{sect:halofinding}
The data output by a cosmological simulation consists of individual particles tracing the underlying density field. To make comparisons to the real universe, a way of matching structures in the simulation to observable objects is needed. In a dark matter only simulation no structure would obviously be directly observable but luckily many properties of dark matter haloes from simulations can be compared with the estimated dark matter haloes of observed galaxies. %TODO cite?
Making such comparisons requires naturally such structures to be identified first, which makes structure finding a key step in the data analysis for many astrophysical and cosmological applications \citep{knebe2013structure}.

A number of different halo finders designed to read N-body data and extract locally overdense gravitationally bound systems have been developed to suit different needs, most based either on locating density peaks or collecting and linking together particles located close to each other based on some metric \citep{knebe2013structure}. A pair of methods belonging to the two groups respectively, the friends-of-friends (FOF) and SUBFIND algorithms, are discussed here. Both algorithms were developed separately, FOF by \citet{davis1985evolution} and SUBFIND by \citet{springel2001populating}, but they work well when used together by inputting FOF results to SUBFIND as a starting point for the subhalo finding \citep{springel2005cosmological}.

The FOF algorithm is simple and based purely on the spatial separation of the particles: pairs of particles residing closer to each other than a chosen threshold distance called linking length are marked to reside within same group by linking them together \citep{davis1985evolution}. When all particles have been processed, each distinct subset of particles linked to each other is defined to be a group \citep{davis1985evolution}. Figure \ref{fig:fof} presents an example of a set of particles grouped using the FOF algorithm. Depending on the specifics of the data and its intended usage, one might want to discard the smallest groups with only a few particles both because they are more likely than bigger groups to be just realizations of the random noise instead of actual physical structures but also due to their small size as they might not be massive enough to represent the structures that are being studied.

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/fof.pdf_tex}
    \caption{Friends-of-friends groups found from a mock data set using the length of the indicator in the upper right area of the illustration as the linking length. Particles are depicted as black dots and the links connecting particles within groups are shown with black lines.}\label{fig:fof}
\end{figure}

The algorithm is made appealing by its simplicity, small number of free parameters and ability to find structures of arbitrary shape \citep{davis1985evolution}. However, it is prone to link unrelated structures via thin bridges that might consist of only a single chain of particles. This behaviour can be seen in figure \ref{fig:fof}: the leftmost group has two dense areas that could be cores of two separate groups connected by a single-particle bridge. Removal or even suitable movement of this particle would result in the group separating into two distinct groups. The two big groups also stretch quite close to each other in their lower parts: moving or adding one particle between the chains of particles protruding towards each other would result in the two groups merging. Also the algorithm as is cannot be used to detect substructure within larger objects, though modifications of it such as hierarchical friends-of-friends algorithm by \citet{Gottlober1999halo} have been developed \cite{springel2001populating}.

In contrast to groups found by FOF, the SUBFIND algorithm has been developed to extract physically well-defined subhaloes that are self-bound and locally overdense from a given parent group. SUBFIND can work with an arbitrary parent group, but FOF groups are well-suited for parent groups as the algorithm is simple and with appropriately long linking lengths the FOF groups, while possible to consist of multiple independent physical structures, are unlikely to split a physical structure between FOF groups \citep{springel2001populating}.

Unlike FOF, SUBFIND uses a local density estimate instead of individual particle pairs. It labels all locally overdense regions enclosed by an isodensity contour traversing a saddle point as substructure candidates \citep{springel2001populating}. This is done by lowering an imaginary density threshold through the range of the density field: particles surrounding a common local density maximum are assigned to a common substructure candidate until two separate substructure regions join in a saddle point of the potential \citep{springel2001populating}. When a saddle point is reached the two substructure candidates it connects are both stored individually to be processed further and the saddle point particle is added to a new substructure candidate containing the particles from both of the smaller candidates \citep{springel2001populating}. Thus the algorithm is able to identify a hierarchy of substructures within each other \citep{springel2001populating}.

A two-dimensional example of this substructure candidate identification is shown in figure \ref{fig:subfind}. The algorithm starts from the particle in the most dense area of the simulation. At that point, the particle has no neighbours that would be in higher density than the particle itself, and thus it becomes the first particle of a substructure candidate. All of the particles belonging to this substructure candidate are marked with dashing in the figure. The algorithm iterates through the particles in order of decreasing density, always finding that the next particle has one neighbouring particle in higher density area than the particle itself and thus adding it to the same dashed group, until the second local density maximum is reached. At that point, a new substructure candidate marked with checkerboard pattern is created. The following particles are assigned to their respective substructure candidates based on their single higher potential neighbour until a saddle point between the two is reached, at which point the state of the substructure candidates is shown in figure \ref{fig:subfind}. At that point the current substructure candidates are complete and will be saved. Next the particles belonging to the two structures can be joined by the saddle point particle to join a new bigger substructure candidate.

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/subfind.pdf_tex}
    \caption{Intermediate stage of the SUBFIND algorithm, shown just before it reaches the first saddle point. Circles depict simulation particles and the line the underlying density field. Striped and checkered circles both belong to their own subhalo candidates whereas the white ones are not yet labeled.}\label{fig:subfind}
\end{figure}

Unfortunately, now some particles are assigned to multiple substructure candidates and it is not clear that all particles within one substructure candidate are actually part of an actual physical structure \citep{springel2001populating}. Is is very much possible that some particles are just passing by and if the same particles were re-examined at a later time, they would no longer be anywhere near the structure they were supposed to belong to. Hence the next step in the analysis is to eliminate unbound particles by iteratively removing particles with positive energy until all of the remaining particles are bound to each other by their mutual gravitational attraction \citep{springel2001populating}. At this stage, each particle is labelled only based on the smallest structure it resides in, which solves the problem of a single particle belonging to multiple structures \citep{springel2001populating}.

After the iterative pruning stage some substructure candidates can vanish completely or be left with very few members. These substructure candidates with less than some minimum number of bound particles can be discarded \citep{springel2001populating}. The structures candidates surviving the pruning can then be considered to represent physical structures and are labelled as subhaloes \citep{springel2001populating}. In this work, all of the analysis is based on catalogues containing such subhaloes. 

\section{Simulation Runs}
[insert some form of attribution for the simulations, e.g. some kind of ''in preparation'' citation?]. The simulations are cosmological zoom-in simulations, meaning that interesting regions at $z=0$ were identified from an output from a low-resolution simulation, after which resolution of the interesting volumes was increased and the simulation was run again. In this case, the interesting regions were defined as surroundings of a pair of dark matter haloes resembling ones of the Milky Way and Andromeda galaxies, the two main galaxies of the Local Group.

The simulations contained only dark matter, and for my analysis only the dark matter halo information was needed, so instead of using the full simulation output I conducted my analysis on subhalo catalogues. The subhalo information was extracted as described in section \ref{sect:halofinding}. The resulting data set consists of subhalo catalogues from 448 zooms, each centered on one region resembling the Local Group in the low-resolution simulation. The following sections shortly introduce the code used to run the simulation and the parameters of the simulation for both stages of the zoom-in simulations.

%Volume, number of particles, compare to other simulations, where better and where maybe worse

%Resimulation of interesting regions

%Simulation has same parameters as EAGLE
%800 Mpc volume used
%schaye 2015 paper
%DM-only parts: Volker-Springer Gadget and Gadget 2 papers 1999 and 2005 or something,     gravity part is more interesting than SPH 
%Zooms can use multiple meshes, only one is used here
%gravitational softening


\subsection{Modified GADGET-3} \label{sect:gadget}
The simulation code that was used to run the simulations on which all of the analysis in this master's thesis is based on was a modified version of GADGET-3, an earlier version of which is described in \citet{springel2005cosmological}. The code is same as used in the EAGLE project, so detailed descriptions of the changes can be found in \citet{schaye2015eagle}. However, the changes mostly affect the handling of baryonic matter so understanding the basic GADGET-2 gives a good basis for understanding the simulations \citep{schaye2015eagle}.

GADGET uses a TreePM algorithm to compute forces, meaning that short-range forces are calculated using a tree method as described in section \ref{sect:tree} and a particle mesh is employed for long-range forces \citep{springel2005cosmological}. As the code is parallel, normal tree-construction algorithm is problematic in regards to splitting the nodes of the octree between processors \citep{springel2005cosmological}. To ensure a balanced workload amongst all processors, the particles are split between processors by constructing a space-filling fractal curve known as Peano-Hilbert curve, and splitting it to segments with approximately equal number of particles on each segment \citep{springel2005cosmological}. The properties of the curve ensure that particles close to each other along the curve are also near each other in the 3D space, which means that close-range forces can frequently be calculated without need to access memory belonging to other processors \citep{springel2005cosmological}. In regions where an octree constructed from particles belonging to a processor should contain particles assigned to other processors, a pseudoparticle resembling in principle the pseudoparticles used when calculating forces for cells where $l/D < \theta$ is inserted instead of the full particle information \citep{springel2005cosmological}.

Updating the positions of the particles is done using an integrator resembling the leapfrog integrator described in section \ref{sect:leapfrog}, but the integrator is modified to allow using variable time-step lengths \citep{springel2005cosmological}. These modifications are important, as in cosmological simulations it is not sensible to use the same time step in all parts of the simulation as there are both high-density regions and sparse void areas, first of which requiring a time-step so small that a lot of computational time is wasted while integrating the latter with more detail than is needed.

TODO: wrap up, maybe by providing the reasoning behind choosing gadget %TODO

\subsection{Parent simulation}
First step of a zoom-in simulation running process is to construct initial conditions and run a low-resolution box. A large box with 800 Mpc/h was chosen to ensure that the box is big enough to contain a reasonable sample of Local Group analogues and that the large scale structure of the universe is represented in the simulation.

TODO: simulation properties, both cosmology and resolution related, starting resolution

\begin{table}
	\centering
	\begin{tabular}{| l | l |}
		\hline
		Box size & 800 Mpc/h \\ \hline
		Number of particles & 0 \\ \hline
		Particle mass & 0 \\ \hline
		Softening length & 0 \\ \hline
	\end{tabular}
	\caption{Properties of the parent simulation.} \label{tab:parentBox}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{| l | l |}
		\hline
		H & 67.77 km/s/Mpc\\ \hline
		$\Omega_m$ & 0.307 \\ \hline
		$\Omega_\Lambda$ & 0.693 \\ \hline
	\end{tabular}
	\caption{Cosmological parameters used in the simulation. TODO: planck + lensing 2013} \label{tab:cosmopars}
\end{table}

When the low-resolution box reached redshift $z=0$, haloes were identified using the procedure described in section \ref{sect:halofinding}. From the resulting halo catalogue, halo pairs resembling the Local Group were identified. TODO: the criteria used %TODO

\subsection{Zoom simulations}


\begin{table}
	\centering
	\begin{tabular}{| l | l | l | l | l |}
		\hline
		& min & mean & median & max \\ \hline
		High resolution particles & min & mean & median & max \\ \hline
		High resolution particle mass  & min & mean & median & max \\ \hline
		High resolution softening length & min & mean & median & max \\ \hline
		Total particles  & min & mean & median & max \\ \hline
	\end{tabular}
	\caption{Properties of the zoom simulations.} \label{tab:parentBox}
\end{table}


\chapter{Mathematical and statistical methods}
täällä tarvittavat esitiedot ja önnönnöö, listaa mm. mitä aiot kertoa kunhan tiedät itsekään

\section{Statistical Background}
\reversemarginpar
\marginnote{\footnotesize{vähän parempi tässä kuin aiemman otsikon alla}}
Precision of the used equipment limits accuracy of all data gathered from physical experiments, simulations or observations. Therefore the results are affected by the measurement process and the results have to be presented as estimates with some error, magnitude of which is affected by both number of data points and accuracy of the measurement equipment \citep{bohm2010introduction}.

Estimating errors for measured quantities offers a way to test hypotheses and compare different experiments \citep{bohm2010introduction}. This is done using different statistical methods, of which the main methods relevant for this thesis are covered here. The methods are shortly introduced in the following sections together with basic statistical concepts that are necessary to understand the methods. 


\subsection{Hypothesis testing and p-values}
A common situation in scientific research is that one has to compare a sample of data points to either a model or another sample in order to derive a conclusion from the dataset. In statistics, this is known as hypothesis testing. For example, this can mean testing hypotheses such as ''these two variables are not correlated'' or ''this sample is from a population with a mean of 1.0'' \citep{wall2003practical}. Next paragraphs shortly introduce the basic concept of hypothesis testing and methods that can used to test the hypothesis ''these two samples are drawn from the same distribution'' following the approach of \citep{bohm2010introduction} and \citep{wall2003practical}.

The process of hypothesis testing as described by begins with forming of a null hypothesis $H_0$ that is formatted such that the aim for the next steps is to either reject it or deduce that it cannot be rejected with a chosen significance level. Negation of the null hypothesis is often called research hypothesis or alternative hypothesis and denoted as $H_1$. For example, this can lead to $H_0$ ''this dataset is sampled from a normal distribution'' and $H_1$ ''this dataset is not sampled from a normal distribution''. Choosing the hypothesis in this manner is done because often the research hypothesis is difficult to define otherwise.

After setting the hypothesis one must choose an appropriate test statistic. Ideally this is chosen such that the difference between cases $H_0$ and $H_1$ is as large as possible. Then one must choose 
the significance level $\alpha$ which corresponds to the probability of rejecting $H_0$ in the case where $H_0$ actually is true. This fixes the critical region i.e.\ the values of test statistic that lead to the rejection of the $H_0$. 

This kind of probability based decision making is always prone to error. It is easy to see that $\alpha$ corresponds to the chance of $H_0$ being rejected when it is true. This is known as error of the first kind. However, this is not the only kind of error possible. It might also occur that $H_0$ is false but it does not get rejected, which is known as error of the second kind.

\reversemarginpar
\marginnote{\footnotesize{kerro mikä $\alpha$ ja N käytössä myöhemmin kunhan tiedät}}
There is no one optimal way of choosing $\alpha$, but instead one should try to find a balance between false rejections of null hypothesis and not being able to reject null hypothesis based on the dataset even if in reality it might not be true. When sample size (often denoted $N$) is large, smaller values of $\alpha$ can often be used as decisions get more accurate when $N$ grows. For example tässä työssä $\alpha$ oli jokin ja $N$ jotain muuta.

It is crucial not to look at the test results before choosing $\alpha$ in order to avoid intentional or unintentional fiddling with the data or changing the criterion of acceptance or rejectance to give desired results. Only after these steps should the test statistic be calculated. If the test statistic falls within the critical region, $H_0$ should be rejected and otherwise stated that $H_0$ cannot be rejected at this significance level. The critical values for different test statistics are widely found in statistical textbooks and collections of statistical tables or they can be calculated using statistical or scientific libraries available for many programming languages.

Despite statistical tests having a binary outcome ''$H_0$ rejected'' or ''$H_0$ not rejected'', a continuous output is often desired. This is what p-values are used for. The name p-value hints towards probability, but despite its name p-value is not equal to the probability that the null hypothesis is true. These p-values are functions of a test statistic and the p-value for a certain value $t_{obs}$ of a test statistic gives the probability that under the condition that $H_0$ is true, the value of a test statistics for a randomly drawn sample is at least as extreme as $t_{obs}$. Therefore if p-value is smaller than $\alpha$, $H_0$ is to be rejected.

\subsection{Distribution functions}
\reversemarginpar
\marginnote{\footnotesize{ei hyvä, harkitse esim http://puppu\-lause\-generaattori.fi/?ava\-in\-sana=ja\-kau\-ma\-funk\-tio}}
Some statistical tests such as the Kolmogorov-Smirnov test and the Anderson-Darling test make use of distribution functions such as cumulative density function (CDF) and empirical distribution function (EDF) in determining the distribution from which a sample is drawn. %Therefore it is important to grasp these concepts in order to fully understand these tests. [this is shit t. puppe]

To understand CDF and EDF, one must first be familiar with probability density function (PDF).
\reversemarginpar
\marginnote{\footnotesize{PDF määritelmä vaikea ymmärtää}}[0.5cm]
As the name suggests, PDF is a function the value of which at some point $x$ represents the likelihood that the value of the random variable would equal $x$. This is often denoted $f(x)$. Naturally for continuous functions the probability of drawing any single value from the distribution is zero, so these values should be interpreted as depicting relative likelihoods of different values. For example if $f(a)=0.3$ and $f(b)=0.6$ we can say that drawing value $b$ is twice as likely as drawing value $a$. \citep{htk}

Another way to use the PDF is to integrate it over semi-closed interval from negative infinity to some value $a$ to obtain the CDF, often denoted with $F(x)$:
\begin{equation}
	F(x) = \int_{-\infty}^x f(x') \,dx'.
\end{equation}	
This gives the probability of a random value drawn from the distribution having value that is smaller than $x$. Relation between the PDF and the CDF is illustrated in figure \ref{fig:cdf}, where PDFs and CDFs are shown for three different distributions. It is easy to see the integral relation between PDF and CDF and how wider distributions have wider CDFs. \citep{htk}

\begin{figure}
   \reversemarginpar
	\marginnote{\footnotesize{lisää johonkin selitys normaalijakauman parametreille}}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/cdf.png}
   \caption{Cumulative distribution function (top panel) for three random samples (PDFs shown in the bottom panel) drawn from different distributions, two of which are normal and one is uniform. Parameters $\mu$ and $\sigma$ of the normal distribution describe the mean and the spread of the distribution respectively, large values of $\sigma$ resulting in wide distribution.}
   \label{fig:cdf}

\end{figure}

\reversemarginpar
\marginnote{\footnotesize{esittelet nyt nolosti EDF:n nimeltä kahdesti, mieti ratkaisu}}
Both the PDF and the CDF apply to whole population or the set of all possible outcomes of a measurement. In reality the sample is almost always smaller than this. Therefore one cannot measure the actual CDF. Nevertheless, it is possible to calculate a similar measure of how big a fraction of measurements falls under a given value. This empirical counterpart of the CDF is known as empirical distribution function (EDF), often denoted $\hat F(x)$, and for a dataset $X_1, X_2,\,..., X_n$ containing $n$ samples it is defined to be
\begin{equation}
	\hat F(x) = \frac{1}{n}\sum_{i=1}^n I[X_i \leq x]
\end{equation}
where $I$ is the indicator function, value of which is 1 if the condition in brackets is true, otherwise 0. \citep{feigelson2012modern}

\reversemarginpar
\marginnote{\footnotesize{harkitse laittavasi jämpti arvo N:lle kun muut osat valmiita}}
Due to the EDF being a result of random sampling, it may deviate from the underlying CDF considerably as can be seen by comparing CDFs in figure \ref{fig:cdf} and corresponding EDFs in figure \ref{fig:edf}. This example is somewhat exaggerated with its $N$=35 as the actual dataset used in this thesis has $N$>100, but reducing the sample size makes seeing the effects of random sampling easier. The latter figure also has EDFs corresponding to two random samples drawn from the distribution of the green curve in the first figure to further illustrate the differences that can arise from random sampling. This randomness also makes determining whether two samples are drawn from the same distribution difficult.

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/edf.png}
   \caption{Empirical distribution function for four random samples ($N$=35) drawn from the same distributions as in figure \ref{fig:cdf}. Note that both the blue and the cyan data are drawn from the same distribution.}
   \label{fig:edf}
\end{figure}


\section{Linear Regression}
Regression analysis is a set of statistical analysis processes that are used to estimate functional relationships between a response variable (denoted with $y$) and one or more predictor variables (denoted with $x$ in case of single predictor or $x_1 \dots x_i$ if there are multiple predictor variables) \citep{feigelson2012modern}. In this section, we will cover both simple regression where there is only one response variable and multiple linear regression where there are more than one response variables. The models also contain $\varepsilon$ term that represents the scatter of measured points around the fit. One of the models used is linear regression model, which can be used to fit any relationship where the response variable is a linear function of the model parameters \citep{montgomery2012introduction}. In addition to the widely known and used models where the relationship is a straight line, such as
\begin{equation}
	y = \beta_0 x + \varepsilon
\end{equation}
all models where relationship is linear in unknown parameters $\beta_i$ are linear \citep{montgomery2012introduction}. Thus for example the following are linear models
\begin{align}
	y &= \beta_0 x^2 + \varepsilon \\
	y &= \beta_0 e^x + \beta_1 \tan{x} + \varepsilon \\
	y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon
\end{align}
On the other hand, all models where the relationship is not linear and therefore
\begin{align}
	y &= x^\beta_0 + \varepsilon \\
	y &= \beta_0 x + \cos{(\beta_1 x)} + \varepsilon
\end{align}
are nonlinear.


\subsection{Simple linear regression}
\reversemarginpar
\marginnote{\footnotesize{onko otsikko järkevä kun }}
Simple linear regression is a model with a single predictor variable and a single response variable with a straight line relationship, i.e.
\begin{equation}
	y = \beta_0 + \beta_1 x + \varepsilon
\end{equation}
where parameter $\beta_0$ represents the $y$ axis intercept of the line and $\beta_1$ is the slope of the line \citep{montgomery2012introduction}. The parameters can be estimated using method of least squares, where such values are found for the parameters that the sum of squared differences between the data points and the fitted line is minimized \citep{montgomery2012introduction}. 

\begin{figure}
   \centering
   \includegraphics[width=0.5\textwidth]{kuvat/OLS-sketch.png}
   \caption{}
   \label{fig:OLS}
\end{figure}

\reversemarginpar
\marginnote{\footnotesize{toisiko jotain lisää jos olisi $\beta_1$ ja $\beta_2$ lausekkeet?}}
The best-known method of minimizing the sum of squared error is the ordinary least-squares (OLS) estimator. The OLS method uses distances measured vertically as shown in figure \ref{fig:OLS} and thus the minimized sum is
\begin{equation}
	\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)
\end{equation}
where $x_1$ and $y_i$ are single values of the measured quantities \citep{feigelson2012modern}. This approach requires that the values of the predictor variable are known exactly without error and all uncertainty is in the values of the response variable \citep{feigelson2012modern}. In those situations where this assumption is not valid, results acquired using OLS may be counterintuitive. This can be seen for example in figure \ref{fig:OLSproblem} where OLS is used to calculate two linear fits: one where $x$ is used and predictor variable and $y$ as response variable and another where $y$ is the predictor and $x$ the response.

\begin{figure}
   \centering
   \includegraphics[width=0.5\textwidth]{kuvat/OLSproblem-sketch.png}
   \caption{}
   \label{fig:OLSproblem}
\end{figure}

\reversemarginpar
\marginnote{\footnotesize{HF: OLS/TLS? Sido PCA:han}}
When dividing the variables to the independent variable with no error and a response variable with possible measurement error is not a justifiable choice OLS should not be used. One alternative for OLS is total least squares (TLS, also known as orthogonal least squares in some sources such as \citep{feigelson2012modern}) regression can be used instead of OLS \citep{markovsky2007overview}.  The major difference between OLS and TLS is that instead of vertical distance, the minimized squared distance is measured between a point and its projection to the fitted line, thus providing minimum of the sum of the squared orthogonal distances from the line \citep{feigelson2012modern}.

\begin{figure}
   \centering
   \includegraphics[width=0.5\textwidth]{kuvat/TLS-sketch.png}
   \caption{}
   \label{fig:TLS}
\end{figure}


\subsection{Multiple linear regression}
\reversemarginpar
\marginnote{\footnotesize{gradun sovellus: onko PCR relevantti?}}
ongelman kuvailu, esim OLS:lle yleistys, jälleen liittyy PCA


%\begin{equation}
%	\matr{y} = \matr{X}\matr{\beta} + \matr{\ varepsilon}
%\end{equation}
%where $y$ is the response variable, $x_i$ are the predictor variables

%Linear Regression Analysis : Theory and Computing
%Introduction to Linear Regression Analysis




%\subsection{Ordinary Least Squares}

%\subsection{


\section{Principal Component Analysis}
\reversemarginpar
\marginnote{\footnotesize{orthogonal vs uncorrelated}}
Principal component analysis (PCA) is a statistical procedure first introdueced by \citet{pearson1901lines} to aid physical, statistical and biological investigations where fitting a line or a plane to n-dimensional dataset is desired. When performing PCA, one transforms a data set to new set of uncorrelated variables i.e.\ ones represented by orthogonal basis vectors. These variables are called principal components (PCs) \citep{jolliffe2002principal}. This approach also solves the problem of sometimes arbitrary choice of division of the data to dependent and independent variables introduced in section 4.2.something \citet{pearson1901lines}. 

\reversemarginpar
\marginnote{\footnotesize{not sure if kannattaa kuvaille proseduuria erityisen tarkkaan, selityksestä tulee pitkällinen jos aikoo aloittaa keskimääräisen tähtitieteilijän lähtötasolta, ehkä vain ohjaa lähteeseen}}
PCA can be used to both reduce and interpret data \citep{johnson2007applied}. Often PCA alone does not produce the desired result, but instead PCs are used as a starting point for other analysis methods such as factor analysis or multiple regression \citep{johnson2007applied}. These applications are introduced in the following subsections together with a short description of performing PCA and interpreting its results. In addition to  these applications, PCA is also used in image compression, face recognition and other fields \citep{smith2002tutorial}.

\subsection{Extracting Principal Components}
\begin{figure}
    \centering
    \input{kuvat/tex/pca-illustrated.pdf_tex}
    \caption{.}\label{fig:pca-illustrated}
\end{figure}

In order to understand the process of obtaining principal components of a data set let us follow the procedure on a two-dimensional data set shown in the top panel of figure \ref{fig:pca-illustrated} with black dots. First step of finding the PCs is to locate the centroid of the dataset i.e.\ the mean of the data along every axis \citep{smith2002tutorial}. This is marked with a red x in the top panel of figure \ref{fig:pca-illustrated}.

\reversemarginpar
\marginnote{\footnotesize{jos päädyt puhumaan eigenvektoreista tai kovarianssimatriiseista, selitä ne täällä}}
The best-fit line and therefore the PCs always pass through the centroid of the system \citep{pearson1901lines}, so subtracting the location of the centroid from the data is a natural next step, as this ensures that in the next step only the slope has to be optimized. This is done in the middle panel of the figure \ref{fig:pca-illustrated}. If the variables have different units, each variable should be scaled to have equal standard deviations \citep{james2013introduction} unless the linear algebra based approach with correlation matrices, as explained in e.g. \citep{jolliffe2002principal}, is used.

If this scaling is not performed, the choice of units can arbitrarily skew the principal components. This is easy to see when considering for example a case where one has distances to galaxies in megaparsecs and their masses in units of $10^{12}\ M_{\astrosun}$, both of which might result in standard deviations being of the order of unity and PCA might thus yield principal components that are not dominated by neither variable alone. Now, say another astronomer has a similar data set, but distances are given in meters. In this case, most of the variation is in the distances, so distances will also dominate the PCs. If all variables are measured in the same units, scaling can be omitted in some cases \citep{james2013introduction}.

Now the first PC can be located by finding the line that passes through the origin and has the maximum variance of the projected data points \citep{jolliffe2002principal}, shown with a black line in the middle panel of figure \ref{fig:pca-illustrated} for our data set. PCs are always orthogonal and intersect at the origin, so in the two-dimensional example case the second and final PC is fully determined. The data set can now be represented using the PCs as is shown in the bottom panel of the figure \ref{fig:pca-illustrated}.

Had the data set had more than two dimensions, the second PC would have been chosen such that it and the first PC are orthogonal and that variance along the new PC is again maximised \citep{jolliffe2002principal}. This can be repeated for each dimension of the data set or, if dimensionality reduction is desired, only for a smaller number of dimensions.

\reversemarginpar
\marginnote{\footnotesize{mieti mitä monospeissillä ja ole konsistentti}}
This level of understanding is often enough to successfully apply PCA to a problem, because PCA has ready-made implementations for many programming languages such as \texttt{prcomp} in R \citep{james2013introduction} and \texttt{sklearn.decomposition.PCA} in scikit-learn library \citep{scikit-learn} for Python. If a more mathematical approach is desired, \citet{smith2002tutorial} explains PCA together with covariance matrices, eigenvectors and eigenvalues required to understand the process very clearly. \citet{jolliffe2002principal} also includes a very thorough description of PCA.


\subsection{Excluding Less Interesting Principal Components}
\reversemarginpar
\marginnote{\footnotesize{maininta bootstrappingista (loppuun?) ja siitä, että maistuu koneoppiminen?}}
Even though a data set has as many principal components as there are measured variables, one is often not interested in all of them as the last principal components might explain only a tiny fraction of the total variation in the data \citep{james2013introduction}. Reducing the dimensionality of the problem also greatly eases visualizing and interpreting the data. Thus one might want to retain only first few of the PCs when PCA is used to for example compress, visualize or just interpret the data set at hand \citep{james2013introduction, johnson2007applied}. Unfortunately, many of the rules and methods used to determine the number of PCs to retain are largely without a formal basis or require assuming a certain distribution which is often not justifiable with the data \citep{jolliffe2002principal}. With careful consideration these methods can nevertheless aid a researcher in making informed decisions and reasoned conclusions, so some rules are introduced in this section.

If the PCA is performed to aid visualizing the data set, retaining only the two first PCs can be a justified choice as two is the maximum number of dimensions that are easy to visualize on two-dimensional media such as paper and the two first PCs determine the best-fit plane for the data \citep{jolliffe2002principal}. Of course the question whether the two PCs are sufficient to describe the data reasonably well still remains unanswered in this case. Fortunately it can be addressed using some of the following methods used in general case of determining how many PCs to retain.

\begin{figure}
    \centering
    \input{kuvat/tex/scree.pdf_tex}
    \caption{Example of a scree plot of randomly generated normally distributed data. In this case the plot has a clear elbow at fifth PC with the PCs 5-9 appearing roughly on a line. Thus the last five PCs could be a good number of PCs to be omitted if dimensionality reduction is desired.}\label{fig:scree}
\end{figure}

One widely used technique was introduced by \citet{cattell1966scree} to be used in factor analysis, but is also very much applicable to PCA \citep{jolliffe2002principal}. This so called Cattell scree test involves plotting the variance of the data points along each PC versus the index of the PC. These plots tend to look similar to what is shown in figure \ref{fig:scree}, resembling a steep cliff with eroded material accumulated at the base, which is why these plots are known as scree plots and the nearly linear section of the plot is called the scree.

When the scree plot has two clearly different areas, the steep slope corresponding to the first PCs and a more gently sloping scree for the latter PCs, locating this elbow in the plot connecting the two areas will give the number of PCs that should be included \citep{jolliffe2002principal}, which in case of figure \ref{fig:scree} would yield five PCs. Some sources such as \citep{cattell1966scree} suggest that in some cases the PC corresponding to the elbow should be discarded, which will result in one less PC.

Unfortunately, as Cattell also acknowledges in his paper, all cases are not as easy to analyze as the one in figure \ref{fig:scree} and may prove difficult to discern for an inexperienced researcher. This problem might arise from for example noise in the linear part of the plot or scree line consisting of two separate linear segments with different slopes. The first case has no easy solution, but in the latter case Cattell suggests using the smaller number of PCs.

\reversemarginpar
\marginnote{\footnotesize{joku kiva lopetus tämän jälkeen?}}
Another straightforward method for choosing how many PCs to retain is to examine how much of the total variation in data is explained by first PCs and including components only up to a point where pre-defined percentage of the total variance is explained \citep{jolliffe2002principal}. Whereas the previous method posed a challenge in determining which PC best matches the exclusion criteria, when using this approach the problem arises from choosing the threshold for including PCs. \citet{jolliffe2002principal} suggests that a value between 70~\% and 90~\% of the total variation is often a reasonable choice, but admits that the properties of the data set may justify values outside this range. Unfortunately, the suggested range is quite wide, so it may contain pultiple PCs and therefore it is up to the researcher to determine the best number of PCs, while the criterion again acts as only an aid in the process. 



\subsection{Principal Component Regression}

\section{Error analysis}

%\section{Goodness of fit testing}
%nyt tarvitsen uuden aloituskappaleen


\section{Comparing two samples drawn from unknown distributions}
\reversemarginpar
\marginnote{\footnotesize{TODO: oispa parempi otsikko. mieti, onko tämä muutenkaan hyvä nyt kun on siirretty yksi otsikkotaso ylöspäin}}[-3cm]
A common question in multiple fields of science is whether two or more samples are drawn from the same distribution. The most relevant methods that can be used to address this problem are introduced here following \citep{bohm2010introduction} and \citep{feigelson2012modern} apart from introducing the $\chi^2$ test which is mostly based on the approach of \citep{corder2014nonparametric}.

Questions related to comparing samples can emerge for example when comparing effectiveness of two procedures, determining if the instrument has changed over time or whether observed data is compatible with simulations. There are multiple two-sample tests that can address this kind of questions, e.g. $\chi^2$, Kolmogorov-Smirnov, Cram\'er-von Mises and Anderson-Darling tests. 

In addition to comparing two samples, these tests can be used as one-sample tests to determine whether it is expected that the sample is from a particular distribution. However, some restrictions apply when using the one-sample variants. Some of these tests use categorical data, i.e. data where variables fall in pre-defined categories, and compares numbers of samples in different categories, whereas the others are applied to numerical data and compare empirical distribution functions (EDF) of the datasets.. Examples of such categories might be for example ''galaxies that are active'' or ''data points between values 1.5 and 1.6''.


\subsection{$\chi^2$ test}
\reversemarginpar
\marginnote{\footnotesize{keksi paremmat esimerkit koko kappaleeseen, jotain relevanttia myöhempää tutkimusta ajatellen. katso kommentit paperista sen jälkeen, kaikkia ei täällä vielä}}
Astronomical data often involves classifying objects into categories such as ''stars with exoplanets'' and ''stars without exoplanets'' or the spectral classes of stars  \citep{feigelson2012modern}. One tool for analyzing such categorical data is $\chi^2$ test, which can be used both to determine whether a sample can be drawn from a certain distribution and to test whether two samples can originate from a single distribution. 

The method described here is sometimes referred to as Pearson's $\chi^2$ test due to existence of other tests where $\chi^2$ distribution is used. In some cases, such as with small $2 \times 2$ contingency tables and when expected cell counts are small, other variants of $\chi^2$ test should be used. For example the Yates's $\chi^2$ test or the Fisher exact test work better in these cases than the $\chi^2$ test.

For one-sample test, the $\chi^2$ test uses the number of measurements in each bin together with a theoretical estimate calculated from the null hypothesis. For example one might have observed exoplanets and tabulated the number of planet-hosting stars of different spectral class as is shown in table \ref{tab:exoplanets} and now wants 
to test the observations against null hypothesis ''Distribution of stellar classes for observed exoplanet-hosting stars is equal to that of main sequence stars in solar neighbourhood as given by \citet{ledrew2001real}'' using significance level $\alpha=0.01$. The data is categorical, so now $\chi^2$ test is a justified choice.

\begin{table}
	\centering
	\begin{tabular}{p{2cm}|p{4cm}}
		Stellar class & Number of observed planetary systems \\ \hline
		A & 6 \\
		F & 38 \\
		G & 39 \\
		K & 134
	\end{tabular}
	\caption{Example of categorical data.}
	\label{tab:exoplanets}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{ l | l | l }
		Stellar class & Observations ($f_o$)& Theory ($f_e$) \\ \hline
		A & 6 & 6 \\
		F & 38 & 28 \\
		G & 39 & 71 \\
		K & 134 & 112 \\ \hline
		total & 217 & 217
	\end{tabular}
	\caption{Data of table \ref{tab:exoplanets} together with expected values if null hypothesis was true.}
	\label{tab:exoplanets-null}
\end{table}

In this case the first step would be to calculate the expected observation counts for each bin according to the null hypothesis. Table \ref{tab:exoplanets-null} contains these expected counts ($f_e$) together with the observations ($f_o$). These observed and expected values are then used to calculate the $\chi^2$ test statistic, defined as
\begin{equation}
	\chi^2 = \sum_i \frac{(f_o - f_e)^2}{f_e}.
\end{equation}
With the data given above this results in $\chi^2 \approx 23.6$. The data has four bins, so the degree of freedom is $4-1=3$. Next one can compare the calculated $\chi^2$ value to a tabulated critical value for our significance level $\alpha=0.01$. These tabulated values can be widely found in statistics textbooks and books specifically dedicated to statistical tables.

In this case according to \citet{corder2014nonparametric} the critical value is 11.34, which means that as $23.6>11.34$ one can reject the null hypothesis and conclude that at 1\% significance level the distribution of stellar classes for observed exoplanet-hosting stars is not equal to that of main sequence stars in solar neighbourhood. This of course can either be due to exoplanets being more numerous around some stellar classes than others or arise from some observational effect such as the observer observing more of the later type stars and thus arbitrarily skewing the distribution of the exoplanet finds.

The $\chi^2$ test can also be used to test for independence of two or more samples. The data is again tabulated and now the $\chi^2$ test statistic is calculated as
\begin{equation}
	\chi^2 = \sum_i \sum_j \frac{(f_{oij}-f_{eij})^2}{f_{eij}}
\end{equation}
where $f_{oij}$ denotes the observed frequency in cell $(i, j)$ and $f_{eij}$ is the expected frequency for that cell. The expected frequency can be calculated using the following formula
\begin{equation}
	f_{eij} = \frac{R_i C_i}{N}
\end{equation}
where $R_i$ is the number of samples in row $i$, $C_j$ is the number of samples in column $j$ and $N$ is the total sample size.

According to \citet{corder2014nonparametric}, the degrees of freedom is $(R-1)(C-1)$ where R is the number of rows and C is the number of columns in tabulated data. This is true in many if not most cases, but the way of collecting data can affect the degrees of freedom in both one-sample and multi-sample cases, as \cite{press2007numerical} explains. For example, if the one-sample model is not renormalized to fit the total number of observed events or, in two-sample case, the sample sizes differ, the degrees of freedom equal to number of bins $N_b$ instead of $N_b-1$.

Before performing the $\chi^2$ test on a dataset, it is important to confirm that the data meets the assumptions for $\chi^2$ test, given for example in \citep{bock2010stats} and \citep{htk}. First of all, the data has to consist of counts i.e.\ not for example percentages or fractions. These counts should be independent of each other and there has to be enough of them, generally $>50$ is sufficient. Bins should also be chosen such that all bins have at least five counts according to the null hypothesis. If the last condition is not met, one can can consider combining bins. 


\subsection{Kolmogorov-Smirnov test}
For astronomers one of the most well-known statistical test is the Kolmogorov-Smirnov test, also known as the KS test. It is computationally inexpensive to calculate, easy to understand and does not require binning of data. It is also a nonparametric test i.e.\ the data does not have to be drawn from a particular distribution.

In the astrophysical context this is often important because astrophysical models usually do not fix a specific statistical distribution for observables and it is common to carry out calculations with logarithms of observables, after which the originally possibly normally distributed residuals  will no longer follow a normal distribution. When using the KS test, the values on the x-axis can be freely reparametrized: for example using $2x$ or $\log x$ on x-axis will result in same value of the test statistic as using just $x$ \citep{press2007numerical}.

The test can be used as either one-sample or two-sample test, both of which are very similar. For two-sample variate the test statistic for the KS test is calculated based on empirical distribution functions $\hat{F}_1$ and $\hat{F}_2$ derived from two samples and the test statistic
\begin{equation}
	D = \sup_{x} |\hat{F}_1(x) - \hat{F}_2(x)|
\end{equation}
uses the maximum vertical distance of the EDFs. This test statistic is then used to determine the p-value and thus decide whether the null hypothesis can be rejected. For one-sample variate the procedure is similar, but EDF $\hat{F}_2$ is substituted with the CDF that corresponds to the null hypothesis.

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/kstest.png}
   \caption{KS test parameter values (magenta vertical lines) shown graphically for three samples from figure \ref{fig:edf}.}
   \label{fig:ks} 
\end{figure}

As an example, let us consider two pairs of samples from figure \ref{fig:edf}: green and blue (two samples drawn from different normal distributions) and blue and cyan (two samples drawn from same normal distribution). We can formulate the test and null hypotheses for both pairs as $H_0$=''the two samples are drawn from the same distribution'' and $H_1$=''the two samples are not drawn from the same distribution'' and choose a significance level of for example $\alpha=0.05$ or $\alpha=0.01$.

\reversemarginpar
\marginnote{\footnotesize{mistä p-value saadaan, kerro taas aiemmin (tai täällä)}}
The test statistic is then calculated and for these samples we get $D=0.51$ for the green-blue pair and $D=0.20$ for the blue-cyan pair. Test statistics are illustrated in figure \ref{fig:ks} where the test statistics $D$ are shown as vertical magenta lines. According to Python function \texttt{scipy.stats.ks\_2samp}, these values of $D$ correspond to p-values $9.9\times 10^{-5}$ and $0.44$ respectively, which means that the null hypothesis ''green and blue samples are drawn from the same distribution'' is rejected at both 0.05 and 0.01 significance levels but the null hypothesis ''blue and cyan samples are drawn from the same distribution'' cannot be rejected.

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/kstest-error.png}
   \caption{KS test ran on another pair of samples drawn from blue and green distributions in figure \ref{fig:cdf}.}
   \label{fig:ks-error} 
\end{figure}

In this case the KS test produced result that matches the actual distributions from which the samples were drawn. Using a different random realization might have resulted in a different conclusion, for example one shown in figure \ref{fig:ks-error} results in $D=0.17$ that corresponds to a p-value of $0.64$ i.e.\ null hypothesis could not have been rejected using the $\alpha$ specified earlier. In a similar manner there can be cases where two samples from one distribution are erroneously determined not to come from the same distribution if the samples differ from each other enough due to random effects.

The latter example case also illustrates one major shortcoming of the KS test: it is not very sensitive to small-scale differences near the tails of the distribution. For example in figure \ref{fig:ks-error} the blue sample goes much further left, but because EDF is always zero at the lowest allowed value and one at the highest one the vertical distances near the tails are small and the test is most sensitive to differences near the median value of the distribution. On the other hand, the test performs quite well when the samples differ globally or have different means. \citep{feigelson2012modern}

\reversemarginpar
\marginnote{\footnotesize{"explain better"}}[1.8cm]
The KS test is also subject to some limitations and it is important to be aware of them in order to avoid misusing it. First of all, the KS test is not distribution free if the model parameters, e.g. mean and standard deviation for normal distribution, are estimated from the dataset that is tested. Thus the tabulated critical values can be used only if model parameters are determined from some other source such as a simulation, theoretical model or another dataset.

Another severe limitation of KS test is that it is only applicable to one-dimensional data. If the dataset has two or more dimensions, there is no unique way of ordering the points to plot EDF and therefore if KS test is used, it is no longer distribution free. Some variants that can handle two or more dimensions have been invented, such as ones by \citet{peacock1983twodimensional} and \citet{fasano1987multidimensional}, but the authors do not provide formal proof of validity of these tests. Despite this, the authors claim that Monte Carlo simulations suggest that the methods work adequately well for most applications.

% mutlidimensionaaleista http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1987MNRAS.225..155F&defaultprint=YES&filetype=.pdf ja http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1983MNRAS.202..615P&defaultprint=YES&filetype=.pdf


\subsection{Other tests based on EDFs}
\reversemarginpar
\marginnote{\footnotesize{ehkä vähän lyhyenpuoleisia kappaleita}}
Unsatisfactory sensitivity of the KS test motivates the use of other more complex tests. Such tests are for example the Cram\'er-von Mises test (CvM) and Anderson-Darling (AD) test, both of which have their strengths. Similar to KS test, both of these can be used as one-sample or two-sample variants.  

First of these tests integrates over the squared difference between the EDF of the sample and CDF from the model or two EDFs in case of two-sample test. The test statistic $W^2$ for one-sample case can be expressed formally as
\begin{equation}
	W^2 = \int_{-\infty}^{\infty}[\hat F_1(x) - F_0(x)]^2\ dF_0(x)
\end{equation}
For two-sample version, the theoretical CDF $F_0$ has to be replaced with another empirical distribution function $\hat F_2$.

Due to integration, the CvM test is able to differentiate distributions based on both local and global differences, which causes it to often perform better than the KS test. Similar to the KS test, the CvM test also suffers from EDFs or an EDF and a CDF being equal at the ends of the data range, which again makes the test less sensitive to differences near the tails of the distribution. 

In order to achieve constant sensitivity over the entire range of values, the statistic has to be weighted according to the proximity of the ends of the distribution. The AD test does this with its test statistic defined as
\begin{equation}
	A^2 = N \int_{-\infty}^{\infty} \frac{[\hat F_1(x) - F_0(x)]^2}{F_0(x)[1-F_0(x)]}\ dF_0(x)
\end{equation}
where $N$ is the number of data points in sample. This weighing makes the test more powerful than the KS and CvM tests in many cases. \citep{bohm2010introduction, feigelson2012modern}

\reversemarginpar
\marginnote{\footnotesize{hnnngh}}
Also other more specific tests exist, such as the Kuiper test which is well suited for cyclic measurements. The test should always be chosen to match the dataset such that it best differentiates between the null and research hypotheses.

\section{Cluster Analysis}
DBSCAN

\chapter{Findings from DMO Halo Catalogue Analysis}
\section{Selection of Local Group analogues}
criteria, how many found, what are like (some plots maybe? distributions of masses, separations, velocity components, number of subhaloes within some radius or correlations between two of those?). Some of this might be part of previous chapter too (relevant to resimulation)?

\reversemarginpar
\marginnote{\footnotesize{TODO: selitykset sille, miten osa on keskittynyt tiettyihin arvoihin sallitulla välillä ja osa jakautunut tasaisemmin. Mieti, pitäisikö kolme viimeistä esittää esim scatterplottina combined mass vs mass in more massive}}
Figure \ref{fig:LGproperties} shows how different features of the found LG analogues are distributed. TODO: selitykset sille, miten osa on keskittynyt tiettyihin arvoihin sallitulla välillä ja osa jakautunut tasaisemmin. 

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/LGproperties.pdf_tex}
    \caption{Distributions of LG analogue properties. TODO: selitä, mieti miten y-akselin label, binien rajat pitäisi pakottaa suurimpaan ja pienimpään sallittuun arvoon}\label{fig:LGproperties}
\end{figure}


\section{Hubble Flow Measurements}
HF, local $H_0$, $H_0$ within shells, zero-point, are previous consistent with what went into the simulation

Figure \ref{fig:hubblediagrams}: two different simulations, MW-centered, huom obs nb how different they are: scatter, number of haloes, changes in scatter (bound structures)

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/hubblediagrams.pdf_tex}
    \caption{Hubble Flows around Milky Way in two simulations.}\label{fig:hubblediagrams}
\end{figure}

Figure \ref{fig:hubblefit} shows haloes included and excluded in fitting, how is the process done

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/hubblefit.pdf_tex}
    \caption{HF slope: 86.9929348817}\label{fig:hubblefit}
\end{figure}

Figure \ref{fig:shelledH0} shows H0 at different radii. First bump is clear, latter ones not, H0>67.7 km/s, why. First ones have ~350 samples, last ones only seven, remember to explain standard error. Figure \ref{fig:zeros} has bigger bins and shows zero points. Think whether both should use samy plot type and which is better (line vs boxplot). If boxplot stays, change colours to all-black? At least explain what is what in plot.

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/shelledH0.pdf_tex}
    \caption{Mean H0 in different 2 Mpc bins, grey curves show standard error.}\label{fig:shelledH0}
\end{figure}

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/zeros.pdf_tex}
    \caption{HF zero point in different 4 Mpc bins. specify one outlier outside the plot}\label{fig:zeros}
\end{figure}


X\section{Anisotropy of Hubble flow}
isotropy + randomness or anisotropy? esittele konsepti. plots: see notebook last pages

\reversemarginpar
\marginnote{\footnotesize{Simulaatio 97, esittele jo tässä näkyvät klöntit joissa paljon samaa väriä, näytä myös klusterointi ja vertaa löytöjä siihen}}
Figure \ref{fig:mollweide-anisotropy} shows distribution of haloes around Milky Way analogue from one simulation with haloes closer than 1.5 Mpc away from center excluded to avoid cluttering the view with Andromeda counterpart and its satellites.

\begin{figure}
    \centering
    %\def\svgwidth{\columnwidth}
    \input{kuvat/tex/mollweide-anisotropy.pdf_tex}
    \caption{Projections of haloes around the less massive LG primary with distances ranging from 1.5 Mpc to 5.0 Mpc.}\label{fig:mollweide-anisotropy}
\end{figure}

\begin{figure}
    \centering
    \input{kuvat/tex/directionalHF.pdf_tex}
    \caption{Mean Hubble flow slope and zero point as seen from Milky Way analogue in different 20\textdegree\ bins as measured from line connecting Milky Way and Andromeda analogues, direction 0\textdegree\ being towards Andromeda.}\label{fig:directionalHF}
\end{figure}

\subsection{Clustering}
Used DBSCAN introduced in [earlier chapter], angular distances of projections on sky as seen from MW.

Figure \ref{fig:clusteringparameters} shows the effect of varying minsamples and $\varepsilon$ on number of clusters found in each simulation (1.5 Mpc < r < 5.0 Mpc again). Regions where there are ridiculously many clusters and ones where there are one or zero, relevant region in between, some areas have similar number of clusters but do the clusters look the same, see plots that don't exist yet

\begin{figure}
    \centering
    \input{kuvat/tex/clusteringParameters.pdf_tex}
    \caption{Mean number of clusters found for all simulations in dataset with different DBSCAN parameters. In all simulations $\varepsilon$ is scaled using the mean distance between closest neighbours.}\label{fig:clusteringparameters}
\end{figure}

\reversemarginpar
\marginnote{\footnotesize{TODO: mieti laitatko samaan figureen, vertaile kuitenkin, selitä vasemmanpuolimmainen pikselisarake}}
Figure \ref{fig:clusterdiameter} shows the change in mean diameter (supremum of angular distance between haloes) in cluster when $\varepsilon$ and minsamples are varied. White areas where no clusters are found in any simulation.

\begin{figure}
    \centering
    \input{kuvat/tex/clusterDiameter.pdf_tex}
    \caption{Mean diameter of clusters found for all simulations in dataset with different DBSCAN parameters. In all simulations $\varepsilon$ is scaled using the mean distance between closest neighbours.}\label{fig:clusterdiameter}
\end{figure}

\reversemarginpar
\marginnote{\footnotesize{Ehkä vähän vähemmän tilaa plottien välissä vaakasuunnassa? Keltaiset vähän turhan samanlaisia.}}
Figures \ref{fig:clusteringExamples} and \ref{fig:clusteringvariations} show how the clustering results vary when clustering parameters are varied.

\begin{figure}
    \centering
    \input{kuvat/tex/clusteringExamples.pdf_tex}
    \caption{Results of DBSCAN clustering on same simulation output with different clustering parameters. TODO: mieti, kuuluuko tämäntyyppinen DBSCANin yleisiä ominaisuuksia esittelevä kuva enemmänkin teoriaosaan. Toisaalta selvästi dataspesifejä juttuja.}\label{fig:clusteringExamples}
\end{figure}

\begin{figure}
    \centering
    \input{kuvat/tex/smallClusteringVariations.pdf_tex}
    \caption{The effect of slightly variying the clustering parameters around the values $\varepsilon$=1.8 and minsamples=10 used when analyzing clustered data.}\label{fig:clusteringvariations}
\end{figure}

\reversemarginpar
\marginnote{\footnotesize{Massakynnys? Kaksi eri kynnystä? Liian kapea ja epätasapainoinen, laita päällekkäin?}}
Figure \ref{fig:clusteredHFparameters} shows how derived values of slope and zero-point for the Hubble flow change when the Hubble flow fitting is carried out on partial data chosen based on the cluster membership of the haloes.

\begin{figure}
    \centering
    \input{kuvat/tex/clusteredHFparameters.pdf_tex}
    \caption{Hubble constant and distance to the point at which velocity due to the fitted Hubble flow is zero calculated from different samples. HUOM OBS NB erittele plotin ulkopuolelle jääneet kaukaiset outlierit}\label{fig:clusteredHFparameters}
\end{figure}

\section{Statistical Estimate of the Local Group Mass}
Analysis similar to Fattahi et al 2016 paper



%\chapter{SIBELIUS project}
% Simulations Beyond the Local Universe
%
%\section{Hubble Flow Fitting}
%
% %if results
%\chapter{Conclusions}

\chapter{Conclusions}





% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\clearpage
\addcontentsline{toc}{chapter}{Bibliography} % This lines adds the bibliography to the ToC
\bibliographystyle{plainnat}
\bibliography{lahteet}

\newpage
\appendix
\chapter{Principal Components}

\begin{table}
	\centering
	\rotatebox{90}{
	\begin{tabular}{l | l | l l l | l l l | l l l}
      \thead{PC} & \thead{$H_0$} & \thead{HF zero} & \thead{HF zero\\(clustered)} & \thead{HF zero\\(not clustered)} & \thead{$\sigma_{radvel}$} & \thead{$\sigma_{radvel}$\\(clustered)} & \thead{$\sigma_{radvel}$\\(not clustered)} & \thead{$v_{r, LG}$} & \thead{$v_{t, LG}$} & \thead{$r_LG$}\\
      \hline
	\makecell{1} & \makecell{-0.386} & \makecell{-0.449} & \makecell{-0.324} & \makecell{-0.379} & \makecell{-0.393} & \makecell{-0.211} & \makecell{-0.384} & \makecell{-0.211} & \makecell{0.097} & \makecell{0.013}\\
	\makecell{2} & \makecell{0.147} & \makecell{0.221} & \makecell{0.248} & \makecell{0.150} & \makecell{-0.064} & \makecell{-0.599} & \makecell{-0.056} & \makecell{-0.599} & \makecell{-0.103} & \makecell{0.332}\\
	\makecell{3} & \makecell{0.287} & \makecell{0.145} & \makecell{0.186} & \makecell{0.223} & \makecell{-0.531} & \makecell{0.258} & \makecell{-0.535} & \makecell{0.258} & \makecell{0.115} & \makecell{0.313}\\
	\makecell{4} & \makecell{0.009} & \makecell{0.020} & \makecell{-0.152} & \makecell{0.102} & \makecell{0.151} & \makecell{-0.047} & \makecell{0.136} & \makecell{-0.047} & \makecell{0.934} & \makecell{0.221}\\
	\makecell{5} & \makecell{-0.024} & \makecell{-0.171} & \makecell{-0.273} & \makecell{-0.105} & \makecell{0.140} & \makecell{0.134} & \makecell{0.227} & \makecell{0.134} & \makecell{-0.270} & \makecell{0.840}\\
	\makecell{6} & \makecell{0.015} & \makecell{-0.092} & \makecell{0.706} & \makecell{-0.663} & \makecell{0.049} & \makecell{0.065} & \makecell{0.072} & \makecell{0.065} & \makecell{0.147} & \makecell{0.127}\\
	\makecell{7} & \makecell{-0.848} & \makecell{0.156} & \makecell{0.323} & \makecell{0.343} & \makecell{-0.074} & \makecell{0.071} & \makecell{0.060} & \makecell{0.071} & \makecell{-0.001} & \makecell{0.128}\\
	\makecell{8} & \makecell{-0.162} & \makecell{0.582} & \makecell{-0.206} & \makecell{-0.315} & \makecell{0.456} & \makecell{0.024} & \makecell{-0.529} & \makecell{0.024} & \makecell{-0.018} & \makecell{0.061}\\
	\makecell{9} & \makecell{0.041} & \makecell{-0.572} & \makecell{0.239} & \makecell{0.326} & \makecell{0.549} & \makecell{-0.008} & \makecell{-0.452} & \makecell{-0.008} & \makecell{-0.016} & \makecell{0.031}\\
	\makecell{10} & \makecell{0.000} & \makecell{-0.000} & \makecell{0.000} & \makecell{0.000} & \makecell{-0.000} & \makecell{0.707} & \makecell{-0.000} & \makecell{-0.707} & \makecell{0.000} & \makecell{-0.000} 
	\end{tabular}
	}
	\caption{component	H0s	zeropoints	inClusterZeros	outClusterZeros	allDispersions	clusterDispersions	unclusteredDispersions	radialVelocities	tangentialVelocities	LGdistances
}\label{tab:PCs}
\end{table}

\end{document}

