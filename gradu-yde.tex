% STEP 1: Choose oneside or twoside. Use the 'draft' option a lot when writing.
\documentclass[english, oneside]{HYgradu}

\usepackage[utf8]{inputenc} % For UTF8 support. Use UTF8 when saving your file.
\usepackage{lmodern} % Font package
\usepackage{textcomp}
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
%\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage[square,sort,colon]{natbib} % For bibliography
\usepackage[bf]{caption} % For more control over figure captions
\usepackage{marginnote} % margin notes for myself
\usepackage{titlesec}
\fussy % Probably not needed but you never know...

\renewcommand{\topfraction}{.75} % less single-float pages
\renewcommand{\floatpagefraction}{.75} % less single-float pages


% näillä subsubsubsection käyttöön
%\titleclass{\subsubsubsection}{straight}[\subsection]
%\newcounter{subsubsubsection}[subsubsection]
%\renewcommand\thesubsubsubsection{\thesubsection.\arabic{subsubsubsection}}
%\titleformat{\subsubsubsection}
%  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}
%\titlespacing*{\subsubsubsection}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
%\makeatletter
%\def\toclevel@subsubsubsection{3}
%\def\l@subsubsubsection{\@dottedtocline{4}{7em}{4em}}
%\@addtoreset{subsubsubsection}{section}
%\@addtoreset{subsubsubsection}{subsection}
%\makeatother
%\setcounter{secnumdepth}{3}
%\setcounter{tocdepth}{4}

% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
%\hypersetup{
%    bookmarks=true,         % show bookmarks bar first?
%    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
%    pdftoolbar=true,        % show Acrobat’s toolbar?
%    pdfmenubar=true,        % show Acrobat’s menu?
%    pdffitwindow=false,     % window fit to page when opened
%    pdfstartview={FitH},    % fits the width of the page to the window
%    pdftitle={},            % title
%    pdfauthor={},           % author
%    pdfsubject={},          % subject of the document
%    pdfcreator={},          % creator of the document
%    pdfproducer={pdfLaTeX}, % producer of the document
%    pdfkeywords={something} {something else}, % list of keywords for
%    pdfnewwindow=true,      % links in new window
%    colorlinks=true,        % false: boxed links; true: colored links
%    linkcolor=black,        % color of internal links
%    citecolor=black,        % color of links to bibliography
%    filecolor=magenta,      % color of file links
%    urlcolor=cyan           % color of external links
%}

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Your Title Here}
\author{Anni Järvenpää}
\date{\today}
\level{Master's thesis}
\faculty{Faculty of Science}
\department{Department of Physics}
\address{PL 64 (Gustaf Hällströmin katu 2a)\\00014 University of Helsinki}
\subject{Astronomy}
\prof{Associate Professor Peter Johansson}{Dr. Till Sawala}
\censors{prof. Smith}{doc. Smythe}{}
\depositeplace{}
\additionalinformation{}
\numberofpagesinformation{\numberofpages\ pages}
\classification{}
\keywords{Your keywords here}
\quoting{}%``Bachelor's degrees make pretty good placemats if you get them laminated.'' \\---Jeph Jacques}
% There is a way out of every box, a solution to every puzzle; it's just a matter of finding it
% Things are only impossible until they're not

\begin{document}

% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.
\begin{abstract}
Abstract goes here.
\end{abstract}

% Place ToC
\mytableofcontents



% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Introduction}

\section{TL;DR version of prerequisite information}
\begin{enumerate}
	\item galaxies form
	\begin{itemize}
		\item Why?
		\item When?
		\item How?
		\item Where?
	\end{itemize}
	\item galaxies form in groups
	\item our local group is one of these
	\item something about large scale distribution of galaxies
\end{enumerate}

\section{History of Local Group Research}
LG objects visible with naked eye -> realization they are something outside our galaxy -> realization they are something very much like our galaxy

First determining distance was difficult, now mass is more interesting question

\section{Aim of This Thesis}
Whatever the main results end up being, presented in somewhat coherent manner and hopefully sugar-coated enough to sound Important and Exciting.


\chapter{Theoretical Background}
Think whether LG or LCDM first
%theorythingies from Fattahi paper introduction
\section{Local Group}
Definition of galaxy group, our local group is one of these.

Mass estimate (Li, Yang masses for the LG and MW)

Maybe something about scale of things in our universe, what are galaxy groups made of, what do you get if you go one distance scale up, what's different in galaxy clusters

\subsection{Structure}
Galaxies that are part of LG, distribution of smaller ones around bigger ones

Current mass estimates (at least timing argument, hubble flow and maybe satellites)

\subsection{Evolution}
How have we ended up in a situation described earlier? What will happen in future?


\section{Expanding universe}
\subsection{Discovery}
Make maths, add cosmological constant, make observations, remove cosmological constant

Enough cosmology here or in other sections to make other parts of thesis to make sense and to suffice as master's thesis = basic textbook cosmology and galaxy formation theory %t. puppe
\subsection{$\Lambda$CDM Cosmology}


\subsection{Hubble flow}
What is, where seen, what means, how to measure, hotness/coldness

Plot: observations with fitted hubble flow


\chapter{Mathematical and statistical methods}
\reversemarginpar
\marginnote{\footnotesize{TODO: tämä ei sovi tänne yksinään/ muutoksitta}}
Precision of the used equipment limits accuracy of all data gathered from physical experiments, simulations or observations. Therefore the results are affected by the measurement process and the results have to be presented as estimates with some error, magnitude of which is affected by both number of data points and accuracy of the measurement equipment. \citep{bohm2010introduction}

Estimating errors for measured quantities offers a way to test hypotheses and compare different experiments\citep{bohm2010introduction}. This is done using different statistical methods, a few of which are covered here. Methods used in this work are shortly introduced in the following sections together with basic statistical concepts that are necessary to understand the methods.

\section{Statistical Background}
täällä tarvittavat esitiedot ja önnönnöö, listaa mm. mitä aiot kertoa kunhan tiedät itsekään

\subsection{Hypothesis testing and p-values}
A common situation in scientific research is that one has to compare a sample to either a model or another sample in order to derive a conclusion from the dataset. In statistics, this is known as hypothesis testing. For example, this can mean testing hypotheses like ''these two variables are not correlated'' or ''this sample is from a population with a mean of 1.0''. \citep{wall2003practical} Next paragraphs shortly introduce the basic concept of hypothesis testing and methods that can used to test the hypothesis ''these two samples are drawn from the same distribution''.

Typically the process of hypothesis testing begins with forming of null hypothesis $H_0$ that is formatted such that the aim for the next steps is to either reject it or deduce that it cannot be rejected with a chosen significance level. Negation of the null hypothesis is often called research hypothesis or alternative hypothesis and denoted as $H_1$. For example, this can lead to $H_0$ ''this dataset is sampled from a normal distribution'' and $H_1$ ''this dataset is not sampled from a normal distribution''. Choosing the hypothesis in this manner is done because often the research hypothesis is difficult to define otherwise. \citep{bohm2010introduction, wall2003practical}

After setting the hypothesis one must choose an appropriate test statistic. Ideally this is chosen such that the difference between cases $H_0$ and $H_1$ is as large as possible. Then one must choose 
the significance level $\alpha$ which corresponds to the probability of rejecting $H_0$ in the case where $H_0$ actually is true. This fixes the critical region i.e. the values of test statistic that lead to the rejection of the $H_0$. \citep{bohm2010introduction, wall2003practical}

It is crucial not to look at the test results before choosing $\alpha$ in order to avoid intentional or unintentional fiddling with the data or changing the criterion of acceptance or rejectance to give desired results. Only after these steps should the test statistic be calculated. If the test statistic falls within the critical region, $H_0$ should be rejected and otherwise stated that $H_0$ cannot be rejected at this significance level. \citep{bohm2010introduction, wall2003practical}

This kind of probability based decision making is always prone to error. It is easy to see that $\alpha$ corresponds to the chance of $H_0$ being rejected when it is true. This is known as error of the first kind. However, this is not the only kind of error possible. It might also occur that $H_0$ is false but it does not get rejected, which is known as error of the second kind. \citep{bohm2010introduction}

Despite statistical tests having a binary outcome ''$H_0$ rejected'' or ''$H_0$ not rejected'', a continuous output is often desired. This is what p-values are used for. The name p-value hints towards probability, but despite it's name p-value is not equal to the probability that the null hypothesis is true. These p-values are functions of test statistic and the p-value for a certain value $t_{obs}$ of test statistic gives the probability that under the condition that $H_0$ is true, the value of a test statistics for a randomly drawn sample is at least as extreme as $t_{obs}$. Therefore if p-value is smaller than $\alpha$, $H_0$ is to be rejected. \citep{bohm2010introduction}

\subsection{Distribution functions}
%To understand EDF, one must first be familiar with probability density function (PDF) and cumulative distribution function (CDF).
%TODO
[insert lyhyt aloituskappale here]

As the name suggests, PDF is a function the value of which at some point $x$ represents the likelihood that the value of the random variable would equal $x$. This is often denoted $f(x)$. Naturally for continuous functions the probability of drawing any single value from the distribution is zero, so these values should be interpreted as depicting relative likelihood of different values. For example if $f(a)=0.3$ and $f(b)=0.6$ we can say that drawing value $a$ is twice as likely as drawing value $b$. \citep{htk}

Another way to use the PDF is to integrate it over semi closed interval from negative infinity to some value $a$ to obtain CDF, often denoted with $F(x)$:
\begin{equation}
	F(x) = \int_{-\infty}^x f(x') \,dx'.
\end{equation}	
This gives the probability of a random value drawn from the distribution having value that is smaller than $x$. Relation between PDF and CDF is illustrated in figure \ref{fig:cdf}, where PDFs and CDFs are shown for three different distributions. It is easy to see the integral relation between PDF and CDF and how wider distributions have wider CDFs.\citep{htk}

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/cdf.png}
   \caption{Cumulative distribution function (top panel) for three random samples (PDFs shown on bottom panel) drawn from different distributions, two of which are normal and one is uniform.}
   \label{fig:cdf}
\end{figure}

Both PDF and CDF both apply to whole population or the set of all possible outcomes of a measurement. In reality the sample is almost always smaller than this. Therefore one cannot measure the actual CDF. Nevertheless, it is possible to calculate a similar measure of how big a fraction of measurements falls under a given value. This empirical counterpart of the CDF is known as empirical distribution function, often denoted $\hat F(x)$, and for a dataset $X_1, X_2,\,..., X_n$ containing $n$ samples it is defined to be
\begin{equation}
	\hat F(x) = \frac{1}{n}\sum_{i=1}^n I[X_i \leq x]
\end{equation}
where $I$ is the indicator function, value of which is 1 if the condition in brackets is true, otherwise 0. \citep{feigelson2012modern}

Due to EDF being a result of random sampling, it may deviate from the underlying CDF considerably as can be seen by comparing CDFs in figure \ref{fig:cdf} and corresponding EDFs in figure \ref{fig:edf}. Latter figure also has EDFs corresponding to two random samples drawn from the distribution of the green curve in the first figure to illustrate the differences that can arise from random sampling. This randomness also makes determining whether two samples are drawn from same distribution difficult.

%TODO mieti miten näytät CDF ja EDF relevanssin, lähestyvät toisiaan kun n kasvaa, mites plottihommat?

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/edf.png}
   \caption{Empirical distribution function for four random samples (N=35) drawn from same distributions as in figure \ref{fig:cdf}. Note that both blue and cyan data are drawn from the same distribution.}
   \label{fig:edf}
\end{figure}


\section{Regression Analysis}
line fitting and other trivial things

\section{Error analysis}

%\section{Goodness of fit testing}
%nyt tarvitsen uuden aloituskappaleen

\reversemarginpar
\marginnote{\footnotesize{TODO: oispa parempi otsikko. mieti, onko tämä muutenkaan hyvä nyt kun on siirretty yksi otsikkotaso ylöspäin}}[2cm]
\section{Comparing two samples drawn from unknown distributions}
A common question in multiple fields of science is whether two or more samples are drawn from the same distribution. This can occur for example when comparing effectiveness of two procedures, determining if instrument has changed over time or whether observed data is compatible with simulations. There are multiple two-sample tests that can address this kind of questions, e.g. $\chi^2$, Kolmogorov-Smirnov, Cram\'er-von Mises and Anderson-Darling tests. \citep{bohm2010introduction, feigelson2012modern}

In addition to comparing two samples, these tests can be used as one-sample tests to determine whether it is expected that the sample is from a particular distribution. However, some restrictions apply when using the one-sample variants. Some of these tests use categorical data, for example ''number of galaxies that are active'' or ''number of data points between values 1.5 and 1.6'' and compares numbers of samples in different categories, whereas the others are applied to numerical data and compare empirical distribution functions (EDF) of the datasets. \citep{feigelson2012modern}


\subsection{$\chi^2$ test}
Atronomical data often involves classifying objects into categories such as ''stars with exoplanets'' and ''stars without exoplanets'' or spectral classes of stars. A good tool for analyzing such categorical data is $\chi^2$ test. It can be used both to determine whether a sample can be drawn from a certain distribution and to test whether two samples can originate from a single distribution. \cite{feigelson2012modern, corder2014nonparametric}

For one-sample test, the $\chi^2$ test uses the number of measurements in each bin together with a theoretical estimate calculated from the null hypothesis. For example if one has observed exoplanets and tabulated the number of planet-hosting stars of different spectral class as is shown in table \ref{tab:exoplanets} and wants 
to test the observations against null hypothesis 
\reversemarginpar
\marginnote{\footnotesize{Mieti onko esimerkki tarpeeksi hyvä, p-arvosta tulee ridikulöösin pieni}}
''Distribution of stellar classes for exoplanet-hosting stars is equal to that of main sequence stars in solar neighbourhood as given by \citet{ledrew2001real}'' using significance level $\alpha$~=~0.01, the first step would be to calculate the expected observation counts for each bin according to the null hypothesis. \cite{corder2014nonparametric}

\begin{table}
	\centering
	\begin{tabular}{p{2cm}|p{4cm}}
		Stellar class & Number of observed planetary systems \\ \hline
		A & 350 \\
		F & 1 400 \\
		G & 1 300 \\
		K & 20 000
	\end{tabular}
	\caption{Example of categorical data.}
	\label{tab:exoplanets}
\end{table}

\begin{table}
	\centering
	\begin{tabular}{ l | l | l }
		Stellar class & Observations ($f_o$)& Theory ($f_e$) \\ \hline
		A & 350 & 597 \\
		F & 1 400 & 2 986 \\
		G & 1 300 & 7 524 \\
		K & 20 000 & 11943 \\ \hline
		total & 23 050 & 23 050
	\end{tabular}
	\caption{Data of table \ref{tab:exoplanets} together with expected values if null hypothesis was true.}
	\label{tab:exoplanets-null}
\end{table}

Table \ref{tab:exoplanets-null} contains these expected counts ($f_e$) together with the observations ($f_o$). These observed and expected values are then used to calculate the $\chi^2$ test statistic, defined as
\begin{equation}
	\chi^2 = \sum_i \frac{(f_o - f_e)^2}{f_e}.
\end{equation}
In this case, the one gets $\chi^2 \approx 11529$. TODO: vapausaste, on merkittävä, johonkin vapausasteista ylipäänsä (numerical methods) %TODO

For two-sample test, the test statistic $\chi$ is calculated

Disclaimer: testejä muitakin, tämä pearsonin, rajoitukset käytölle

\subsection{Kolmogorov-Smirnov test}
For astronomers one of the most well-known of these tests is the Kolmogorov-Smirnov test, also known as the KS test. It is computationally inexpensive to calculate, easy to understand and does not require binning of data. It is also nonparametric test i.e. the data does not have to be drawn from a particular distribution. \cite{feigelson2012modern}

In astrophysical context this is often important because astrophysical models often do not fix a specific statistical distribution for observables and it is common to carry out calculations with logarithms of observables, after which the originally possibly normally distributed residuals  will no longer follow normal distribution. When using the KS test, the values on the x-axis can be freely reparametrized: for example using $2x$ or $\log x$ on x-axis will result in same value of the test statistic as using $x$. \cite{feigelson2012modern, press2007numerical}

The test can be used as either one-sample or two-sample test, both of which are very similar. For two-sample variate the test statistic for the KS test is calculated based on empirical distribution functions $\hat{F}_1$ and $\hat{F}_2$ derived from two samples and the test statistic
\begin{equation}
	D = \sup_{x} |\hat{F}_1(x) - \hat{F}_2(x)|
\end{equation}
uses the maximum vertical distance of the e.d.f's. This test statistic is then used to determine the p-value and thus decide whether the null hypothesis can be rejected. For one-sample variate the procedure is similar, but EDF $\hat{F}_2$ is substituted with the CDF that corresponds to the null hypothesis. \citep{feigelson2012modern, bohm2010introduction} %TODO feigelssonin rajoitukset KS:lle ja muille, kerro tarkemmin tuolla mutta hinttaa jo tässä olemassaolosta

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/kstest.png}
   \caption{KS test parameter values (magenta vertical lines) shown graphically for three samples from figure \ref{fig:edf}.}
   \label{fig:ks} 
\end{figure}

As an example, let's consider two pairs of samples from figure \ref{fig:edf}: green and blue (two samples drawn from different normal distributions) and blue and cyan (two samples drawn from same normal distribution). We can formulate the test and null hypotheses for both pairs as $H_0$=''the two samples are drawn from the same distribution'' and $H_1$=''the two samples are not drawn from the same distribution'' and choose a significance level of for example $\alpha=0.05$ or $\alpha=0.01$.

The test statistic is then calculated and for these samples we get $D=0.51$ for the green-blue pair and $D=0.20$ for the blue-cyan pair. Test statistics are illustrated in figure \ref{fig:ks} where the test statistics $D$ are shown as vertical magenta lines. These values of $D$ correspond to p-values $9.9\times 10^{-5}$ and $0.44$ respectively, which means that the null hypothesis ''green and blue samples are drawn from the same distribution'' is rejected at both 0.05 and 0.01 significance levels but the null hypothesis ''blue and cyan samples are drawn from the same distribution'' cannot be rejected.

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{kuvat/kstest-error.png}
   \caption{KS test ran on another pair of samples drawn from blue and green distributions in figure \ref{fig:cdf}.}
   \label{fig:ks-error} 
\end{figure}

In this case the KS test produced result that matches the actual distributions from which the samples are drawn from. Using a different sample might have resulted in different conclusion, for example one shown in figure \ref{fig:ks-error} results in $D=0.17$ that corresponds to p-value of $0.64$ i.e. null hypothesis could not have been rejected using $\alpha$ specified earlier. In a similar manner there can be cases where two samples from one distribution are erroneously determined not to come from the same distribution if the samples differ from each other enough due to random effects.

The latter example case also illustrates one major shortcoming of the KS test: it is not very sensitive to small-scale differences near the tails of the distribution. For example in figure \ref{fig:ks-error} the blue sample goes much further left, but because EDF is always zero at the lowest allowed value and one at the highest one the vertical distances near the tails are small and the test is most sensitive to differences near the median value of the distribution. On the other hand, the test performs quite well when the samples differ globally or have different means. \cite{feigelson2012modern}

The KS test is also subject to some limitations and it is important to be aware of them in order to avoid misusing it. First of all, the KS test is not distribution free if the model parameters, e.g. mean and standard deviation for normal distribution, are estimated from the dataset that is tested. Thus the tabulated critical values can be used only if model parameters are determined from some other source such as a simulation, theoretical model or another dataset. \cite{feigelson2012modern}

Another severe limitation of KS test is that it is only applicable to one-dimensional data. If the dataset has two or more dimensions, there is no unique way of ordering the points to plot EDF and therefore if KS test is used, it is no longer distribution free. Some variants that can handle two or more dimensions have been invented, such as ones by \citet{peacock1983twodimensional} and \citet{fasano1987multidimensional}, but the authors do not provide formal proof of validity of these tests. Despite this, the authors claim that Monte Carlo simulations suggest that the methods work adequatly well for most applications. \cite{press2007numerical}

% mutlidimensionaaleista http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1987MNRAS.225..155F&defaultprint=YES&filetype=.pdf ja http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1983MNRAS.202..615P&defaultprint=YES&filetype=.pdf


\section{Cluster Analysis}
DBSCAN

\chapter{general simulation thingies}
Data used here from EAGLE which uses modified GADGET-2 which is a tree-code that uses leapfrog, other integrators also briefly introduced?
\section{N-body simulations}
\subsection{Hierarchical Tree Algorithm}
\subsection{Numerical Integrators}
\subsection{Halo Finding with Subfind} %iannuzzi thesis
%merger tree mentioned in todo, is it relevant?

\section{Description of actual simulations used}
Volume, number of particles, compare to other simulations, where better and where maybe worse

Resimulation of interesting regions

Simulation has same parameters as EAGLE
800 Mpc volume used
schaye 2015 paper
DM-only parts: Volker-Springer Gadget and Gadget 2 papers 1999 and 2005 or something,     gravity part is more interesting than SPH 
Zooms can use multiple meshes, only one is used here
gravitational softening

\chapter{Findings from DMO Halo Catalogue Analysis}
\section{Selection of Local Group analogues}
criteria, how many found, what are like (some plots maybe? distributions of masses, separations, velocities or correlations between two of those?). This might be part of previous chapter too (relevant to resimulation)?

\section{Local Anisotropy of the Hubble Flow}
Hopefully there's something at least mildly interesting to report when I get to look at the new data

\subsection{Hubble Flow Fitting}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/hubbleflow.png}
%   \caption{Radial velocities of haloes as a function of distance. Best fit to Hubble flow shown with solid line. Nearby points ignored when fitting shown in gray.}\label{fig:hubbleflow}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/hubbleflow-colour.png}
%   \caption{Hubble flow with colours depicting angular disstance from line connecting Milky Way and Andromeda counterparts in simulation.}\label{fig:hubbleflow-colour}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/velocitydispersion.png}
%   \caption{Velocity dispersion of Hubble flow.}\label{fig:velocitydispersion}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/anisotropy-aligned.png}
%   \caption{Haloes with distances between 2 and 5 Mpc as seen from Mily Way counterpart in simulation. Colours depict deviations from best linear Hubble flow fit ignoring haloes up to 2 Mpc away, blue end meaning haloes coming closer faster than expected and redder colours moving away.}\label{fig:anisotropymap}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/clustering-ms5eps2.png}
%   \caption{Dark matter haloes with distances from 2 to 5 Mpc grouped to clusters using DBSCAN clustering algorithm. Parameters used for this plot were ms=5 and eps=2.}\label{fig:clustering}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/eps-02.png}
%   \caption{Median number of clusters found with constant eps on different minsamples.}\label{fig:epseffect}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/ms-5.png}
%   \caption{Mean number of clusters found with constant ms on different eps.}\label{fig:mseffect}
%\end{figure}
%
%\begin{figure}
%   \centering
%   \includegraphics[width=\textwidth]{kuvat/diststd-113.png}
%   \caption{Standard deviation of velocities within cluster as a function of distance.}\label{fig:diststd}
%\end{figure}

\section{Statistical Estimate of the Local Group Mass}
Analysis similar to Fattahi et al 2016 paper

%\chapter{SIBELIUS project}
% Simulations Beyond the Local Universe
%
%\section{Hubble Flow Fitting}
%
% %if results
%\chapter{Conclusions}

\chapter{Conclusions}





% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\clearpage
\addcontentsline{toc}{chapter}{Bibliography} % This lines adds the bibliography to the ToC
\bibliographystyle{plainnat}
\bibliography{lahteet}


\end{document}

